{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014876,
     "end_time": "2024-06-03T17:01:07.523093",
     "exception": false,
     "start_time": "2024-06-03T17:01:07.508217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n",
    "This starter notebook is provided by the Keras team.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014071,
     "end_time": "2024-06-03T17:01:07.551303",
     "exception": false,
     "start_time": "2024-06-03T17:01:07.537232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Getting Started on LLM Classification Fine Tuning with [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras)\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://i.ibb.co/wJMF5HL/lmsys.png\">\n",
    "</div>\n",
    "\n",
    "In this competition, our aim is to predict which LLM responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs). In other words, the goal of the competition is to predict the preferences of the judges and determine the likelihood that a given prompt/response pair is selected as the winner. This notebook will guide you through the process of fine-tuning the **DebertaV3** model for this competition using the **Shared Weight** strategy with KerasNLP. This strategy is similar to how Multiple Choice Question (MCQ) models are trained. Additionally, we will use mixed precision for faster training and inference.\n",
    "\n",
    "**Did you know**: This notebook is backend-agnostic, which means it supports TensorFlow, PyTorch, and JAX backends. However, the best performance can be achieved with `JAX`. KerasNLP and Keras enable the choice of the preferred backend. Explore further details on [Keras](https://keras.io/keras_3/).\n",
    "\n",
    "**Note**: For a deeper understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014736,
     "end_time": "2024-06-03T17:01:07.580011",
     "exception": false,
     "start_time": "2024-06-03T17:01:07.565275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üìö | Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 14.715162,
     "end_time": "2024-06-03T17:01:22.309231",
     "exception": false,
     "start_time": "2024-06-03T17:01:07.594069",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_nlp\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import plotly.express as px\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Â∞áÈÄôË°åÊîæÂú®Á®ãÂºèÊúÄÂâçÈù¢ÔºåÊõøÊèõ YOUR_API_KEY ÁÇ∫‰Ω†ÁöÑÂØ¶Èöõ key\n",
    "key=\"cfbd7f24413a77f911641acae8914b1dc77e1b83\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014604,
     "end_time": "2024-06-03T17:01:22.338854",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.32425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014156,
     "end_time": "2024-06-03T17:01:22.405533",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.391377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ‚öôÔ∏è | Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.023008,
     "end_time": "2024-06-03T17:01:22.442868",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.41986",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42  # Random seed\n",
    "    preset = \"deberta_v3_extra_small_en\" # Name of pretrained models\n",
    "    sequence_length = 512  # Input sequence length\n",
    "    epochs = 8 # Training epochs\n",
    "    batch_size = 16  # Batch size\n",
    "\n",
    "    # optimizer & scheduler\n",
    "    learning_rate = 5e-6\n",
    "    weight_decay = 1e-4\n",
    "    scheduler = 'cosine'        # we‚Äôll build a cosine decay schedule\n",
    "    warmup_ratio = 0.1          # Áî®Âú® cosine warmup\n",
    "    \n",
    "    # regularization\n",
    "    dropout_rate = 0.2          # head Ë£°ÁöÑ Dropout\n",
    "\n",
    "    # train / fine-tune Á≠ñÁï•\n",
    "    head_only_epochs = 3        # ÂâçÂπæÂÄã epoch Âè™Ë®ìÁ∑¥ head\n",
    "    unfrozen_backbone_layers = 4 # ÊúÄÂæåÂÜçËß£Âáç backbone ÂæåÈù¢ÂπæÂ±§\n",
    "    fine_tune_epochs = 5          # Ëß£ÂÜªÂêéÂÜçËÆ≠ÁöÑËΩÆÊï∞\n",
    "    \n",
    "    # labels\n",
    "    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}\n",
    "    name2label = {v:k for k, v in label2name.items()}\n",
    "    class_labels = list(label2name.keys())\n",
    "    class_names = list(label2name.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014043,
     "end_time": "2024-06-03T17:01:22.471173",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.45713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ‚ôªÔ∏è | Reproducibility \n",
    "Sets value for random seed to produce similar result in each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.021526,
     "end_time": "2024-06-03T17:01:22.506961",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.485435",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(CFG.seed)\n",
    "np.random.seed(CFG.seed)\n",
    "random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01407,
     "end_time": "2024-06-03T17:01:22.535249",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.521179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üßÆ | Mixed Precision\n",
    "\n",
    "In this notebook, we will use mixed precision instead of float32 precision for training and inference to reduce GPU memory usage. This will ultimately allow us to use larger batch sizes, thus reducing our training and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.020673,
     "end_time": "2024-06-03T17:01:22.570123",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.54945",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013868,
     "end_time": "2024-06-03T17:01:22.598204",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.584336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üìÅ | Dataset Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.020401,
     "end_time": "2024-06-03T17:01:22.63286",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.612459",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/llm-classification-finetuning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01414,
     "end_time": "2024-06-03T17:01:22.661285",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.647145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üìñ | Meta Data \n",
    "\n",
    "The competition dataset comprises user interactions from the ChatBot Arena. In each interaction, a judge presents one or more prompts to two different large language models and then indicates which model provided the more satisfactory response. The training data contains `55,000` rows, with an expected `25,000` rows in the test set.\n",
    "\n",
    "## Files\n",
    "\n",
    "### `train.csv`\n",
    "- `id`: Unique identifier for each row.\n",
    "- `model_[a/b]`: Model identity, present in train.csv but not in test.csv.\n",
    "- `prompt`: Input prompt given to both models.\n",
    "- `response_[a/b]`: Model_[a/b]'s response to the prompt.\n",
    "- `winner_model_[a/b/tie]`: Binary columns indicating the judge's selection (ground truth target).\n",
    "\n",
    "### `test.csv`\n",
    "- `id`: Unique identifier for each row.\n",
    "- `prompt`: Input prompt given to both models.\n",
    "- `response_[a/b]`: Model_[a/b]'s response to the prompt.\n",
    "\n",
    "> Note that each interaction may have multiple prompts and responses, but this notebook will use only **one prompt per interaction**. You can choose to use all prompts and responses. Additionally, prompts and responses in the dataframe are provided as string-formatted lists, so they need to be converted to literal lists using `eval()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013838,
     "end_time": "2024-06-03T17:01:22.689785",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.675947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 8.625467,
     "end_time": "2024-06-03T17:01:31.32943",
     "exception": false,
     "start_time": "2024-06-03T17:01:22.703963",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Âª∫Á´ãË∑ØÂæë\n",
    "BASE_PATH = Path(BASE_PATH)  # Â¶ÇÊûú‰Ω†‰πãÂâçÂ∑≤Á∂ìÊòØ strÔºåÈÄôË°åÂèØÊääÂÆÉËΩâÊàê Path\n",
    "train_file = BASE_PATH / \"train.csv\"\n",
    "if not train_file.exists():\n",
    "    raise FileNotFoundError(f\"Êâæ‰∏çÂà∞Ë®ìÁ∑¥Ê™îÊ°àÔºö{train_file}\")\n",
    "\n",
    "# 2. ËÆÄÊ™î‰∏¶ÊâìÊï£\n",
    "df = pd.read_csv(train_file)\n",
    "df = df.sample(frac=1, random_state=CFG.seed).reset_index(drop=True)\n",
    "\n",
    "# 3. ÂÆâÂÖ®Âú∞Ëß£Êûê JSON string ‰∏¶ÂèñÁ¨¨‰∏ÄÂÄãÂÖÉÁ¥†\n",
    "def safe_first_element(json_str: str):\n",
    "    if not isinstance(json_str, str) or len(json_str) == 0:\n",
    "        return \"\"\n",
    "    # Êää 'null' ÊèõÊàê JSON ÂèØÊé•ÂèóÁöÑÁ©∫Â≠ó‰∏≤\n",
    "    cleaned = json_str.replace(\"null\", '\"\"')\n",
    "    try:\n",
    "        arr = json.loads(cleaned)\n",
    "        if isinstance(arr, list) and len(arr) > 0:\n",
    "            return arr[0]\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "df[\"prompt\"]     = df[\"prompt\"].apply(safe_first_element)\n",
    "df[\"response_a\"] = df[\"response_a\"].apply(safe_first_element)\n",
    "df[\"response_b\"] = df[\"response_b\"].apply(safe_first_element)\n",
    "\n",
    "# 4. Ê®ôÁ±§ËΩâÊèõÔºàÂíå‰Ω†ÂéüÊú¨‰∏ÄÊ®£Ôºâ\n",
    "df[\"class_name\"]  = df[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].idxmax(axis=1)\n",
    "df[\"class_label\"] = df[\"class_name\"].map(CFG.name2label)\n",
    "\n",
    "# 5. Ê™¢Ë¶ñÁµêÊûú\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Ë®àÁÆóÁº∫Â§±ÂÄº\n",
    "missing_values = df.isnull().sum()\n",
    "missing_columns = missing_values[missing_values > 0]\n",
    "\n",
    "print(\"Áº∫Â§±ÂÄºÁµ±Ë®àÔºö\")\n",
    "print(missing_values)\n",
    "\n",
    "if not missing_columns.empty:\n",
    "    print(\"\\nÊúâÁº∫Â§±ÂÄºÁöÑÊ¨Ñ‰ΩçÔºö\")\n",
    "    print(missing_columns)\n",
    "\n",
    "# 2. ‰∫íÂãïÂºèÈ°ØÁ§∫Áº∫Â§±ÂÄºË°®Ê†ºÔºàÂèØÊéíÂ∫èÔºèÁØ©ÈÅ∏Ôºâ\n",
    "#from ace_tools import display_dataframe_to_user\n",
    "\n",
    "missing_df = (\n",
    "    missing_values\n",
    "    .rename(\"missing_count\")\n",
    "    .to_frame()\n",
    "    .assign(missing_pct=lambda d: d[\"missing_count\"] / len(df) * 100)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"column\"})\n",
    "    .query(\"missing_count > 0\")\n",
    ")\n",
    "\n",
    "#display_dataframe_to_user(\"Áº∫Â§±ÂÄºÊ¨Ñ‰Ωç‰∏ÄË¶Ω\", missing_df)\n",
    "print(\"\\nÁº∫Â§±ÂÄºÊ¨Ñ‰Ωç‰∏ÄË¶ΩÔºö\")\n",
    "print(missing_df)\n",
    "\n",
    "# 3. ËôïÁêÜÁº∫Â§±ÂÄº\n",
    "#    (a) ÈáùÂ∞çÈóúÈçµÊ¨Ñ‰ΩçÁõ¥Êé•‰∏üÊ£ÑÂê´ NaN ÁöÑÂàó\n",
    "df = df.dropna(subset=[\"prompt\", \"response_a\", \"response_b\"]).reset_index(drop=True)\n",
    "\n",
    "#    (b) ÂÖ∂È§òÊ¨Ñ‰ΩçÁî®Á©∫Â≠ó‰∏≤Â°´Ë£ú\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# 4. ÊúÄÂæåÊ™¢Ë¶ñËôïÁêÜÂæåÁöÑÂâçÂπæÁ≠Ü\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014609,
     "end_time": "2024-06-03T17:01:31.359794",
     "exception": false,
     "start_time": "2024-06-03T17:01:31.345185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.034633,
     "end_time": "2024-06-03T17:01:31.409034",
     "exception": false,
     "start_time": "2024-06-03T17:01:31.374401",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Á¢∫‰øù BASE_PATH ÊòØ Path Áâ©‰ª∂\n",
    "BASE_PATH = Path(BASE_PATH)\n",
    "test_file = BASE_PATH / \"test.csv\"\n",
    "if not test_file.exists():\n",
    "    raise FileNotFoundError(f\"Êâæ‰∏çÂà∞Ê∏¨Ë©¶Ê™îÊ°àÔºö{test_file}\")\n",
    "\n",
    "# 2. ËÆÄÂèñÊ∏¨Ë©¶Ë≥áÊñô\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "# 3. ÂÆâÂÖ®Ëß£ÊûêÂáΩÂºèÔºàÂêåË®ìÁ∑¥ÈõÜÔºâ\n",
    "def safe_first_element(json_str: str):\n",
    "    if not isinstance(json_str, str) or len(json_str) == 0:\n",
    "        return \"\"\n",
    "    cleaned = json_str.replace(\"null\", '\"\"')\n",
    "    try:\n",
    "        arr = json.loads(cleaned)\n",
    "        if isinstance(arr, list) and len(arr) > 0:\n",
    "            return arr[0]\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "# 4. ËôïÁêÜ prompt / response Ê¨Ñ‰Ωç\n",
    "test_df[\"prompt\"]     = test_df[\"prompt\"].apply(safe_first_element)\n",
    "test_df[\"response_a\"] = test_df[\"response_a\"].apply(safe_first_element)\n",
    "test_df[\"response_b\"] = test_df[\"response_b\"].apply(safe_first_element)\n",
    "\n",
    "# 5. Ê™¢Ë¶ñÂâçÂπæÁ≠Ü\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alec Êü•ÁúãË≥áÊñôÁãÄÊÖã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Ë®àÁÆó winner\n",
    "winner = df[['winner_model_a','winner_model_b','winner_tie']].idxmax(axis=1).map({\n",
    "    'winner_model_a':'A_win',\n",
    "    'winner_model_b':'B_win',\n",
    "    'winner_tie':'Tie'\n",
    "})\n",
    "# 2. Ë®àÁÆóÊØî‰æã‰∏¶ÊéíÂ∫è\n",
    "counts = winner.value_counts(normalize=True).sort_index()  \n",
    "\n",
    "# 3. Áπ™Âúñ\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "ax = counts.plot.bar(title='Winner Distribution')\n",
    "ax.set_ylabel('Proportion')\n",
    "\n",
    "# 4. Ê®ôË®ªÁôæÂàÜÊØî\n",
    "for patch in ax.patches:\n",
    "    height = patch.get_height()\n",
    "    ax.annotate(f\"{height:.1%}\",\n",
    "                (patch.get_x() + patch.get_width() / 2, height),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 5. Âç∞Âá∫Êï∏ÂÄº\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pair_stats = (\n",
    "    df\n",
    "    .groupby(['model_a','model_b'])\n",
    "    .agg(\n",
    "        total=('id','size'),\n",
    "        a_wins=('winner_model_a','sum'),\n",
    "        b_wins=('winner_model_b','sum'),\n",
    "        ties=('winner_tie','sum'),\n",
    "    )\n",
    "    .assign(\n",
    "        a_win_rate=lambda d: (d['a_wins']/d['total']).round(3),\n",
    "        b_win_rate=lambda d: (d['b_wins']/d['total']).round(3),\n",
    "        tie_rate=lambda d: (d['ties']/d['total']).round(3),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values('total', ascending=False)\n",
    ")\n",
    "\n",
    "display(pair_stats.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014711,
     "end_time": "2024-06-03T17:01:31.438974",
     "exception": false,
     "start_time": "2024-06-03T17:01:31.424263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Contextualize Response with Prompt\n",
    "\n",
    "In our approach, we will contextualize each response with the prompt instead of using a single prompt for all responses. This means that for each response, we will provide the model with the same set of prompts combined with their respective response (e.g., `(P + R_A)`, `(P + R_B)`, etc.). This approach is similar to the multiple-choice question task in NLP.\n",
    "\n",
    "> Note that some prompts and responses may not be encoded with `utf-8`, resulting in errors when creating the dataloader. In such cases, we will replace them with an empty string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.023596,
     "end_time": "2024-06-03T17:01:31.477321",
     "exception": false,
     "start_time": "2024-06-03T17:01:31.453725",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a function to create options based on the prompt and choices\n",
    "def make_pairs(row):\n",
    "    def clean_text(text: any) -> str:\n",
    "        \"\"\"\n",
    "        Â∞áËº∏ÂÖ•Âº∑Âà∂ËΩâÊàê strÔºåÂÜç‰∏üÊéâÈùûÊ≥ï UTF-8 Â≠óÂÖÉ„ÄÇ\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        return text.encode(\"utf-8\", errors=\"ignore\") \\\n",
    "                   .decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # ‰∏ÄÊ¨°ÊãøÂà∞‰πæÊ∑®ÁöÑ prompt / responses\n",
    "    prompt     = clean_text(row.get(\"prompt\", \"\"))\n",
    "    response_a = clean_text(row.get(\"response_a\", \"\"))\n",
    "    response_b = clean_text(row.get(\"response_b\", \"\"))\n",
    "\n",
    "    # Âà§Êñ∑ÊòØÂê¶ÊúâÈùûÂ≠ó‰∏≤Ëº∏ÂÖ•\n",
    "    row[\"encode_fail\"] = any(not isinstance(row.get(col, None), str)\n",
    "                              for col in [\"prompt\", \"response_a\", \"response_b\"])\n",
    "\n",
    "    # Âª∫Á´ã options\n",
    "    row[\"options\"] = [\n",
    "        f\"Prompt: {prompt}\\n\\nResponse A: {response_a}\",\n",
    "        f\"Prompt: {prompt}\\n\\nResponse B: {response_b}\"\n",
    "    ]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 62.732686,
     "end_time": "2024-06-03T17:02:34.262915",
     "exception": false,
     "start_time": "2024-06-03T17:01:31.530229",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df\n",
    "display(df.head(2))  # Display the first 2 rows of df\n",
    "\n",
    "test_df = test_df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df\n",
    "display(test_df.head(2))  # Display the first 2 rows of df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015064,
     "end_time": "2024-06-03T17:02:34.293411",
     "exception": false,
     "start_time": "2024-06-03T17:02:34.278347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Encoding Fail Statistics\n",
    "\n",
    "Let's examine how many samples have encoding issues. From the code below, we can see that only $1\\%$ of the samples failed to be encoded, while $99\\%$ of the samples don't have any issues. A similar pattern can be expected for the test data as well. Thus, considering empty strings for this small portion of the data will not have much impact on our training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.032427,
     "end_time": "2024-06-03T17:02:34.341044",
     "exception": false,
     "start_time": "2024-06-03T17:02:34.308617",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.encode_fail.value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015466,
     "end_time": "2024-06-03T17:02:34.372077",
     "exception": false,
     "start_time": "2024-06-03T17:02:34.356611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üé® | Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015944,
     "end_time": "2024-06-03T17:02:34.404115",
     "exception": false,
     "start_time": "2024-06-03T17:02:34.388171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LLM Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 1.475495,
     "end_time": "2024-06-03T17:02:35.895386",
     "exception": false,
     "start_time": "2024-06-03T17:02:34.419891",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# 1. Âêà‰Ωµ‰∏¶Ë®àÁÆóÊ¨°Êï∏\n",
    "llm_series = pd.concat([df.model_a, df.model_b], ignore_index=True)\n",
    "llm_counts = (\n",
    "    llm_series\n",
    "    .value_counts()                      # ÂæóÂà∞‰∏ÄÂÄã SeriesÔºåindex ÊòØ LLMÔºåÂÄºÊòØÂá∫ÁèæÊ¨°Êï∏\n",
    "    .rename_axis(\"LLM\")                  # Êää index ÂêçÁ®±Ë®≠ÁÇ∫ \"LLM\"\n",
    "    .reset_index(name=\"Count\")           # Êää Series ËΩâÊàê DataFrameÔºå‰∏¶ÊääÂÄºÂàóÂëΩÂêçÁÇ∫ \"Count\"\n",
    ")\n",
    "# 2. ÊéíÂ∫èÔºàÈôçÂÜ™Ôºâ\n",
    "llm_counts = llm_counts.sort_values(by=\"Count\", ascending=False)\n",
    "print(llm_counts.columns)\n",
    "\n",
    "# 3. Áï´Âúñ\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(llm_counts[\"LLM\"], llm_counts[\"Count\"])\n",
    "for bar in bars:\n",
    "    h = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        h,\n",
    "        f\"{h}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.title(\"Distribution of LLMs\")\n",
    "plt.xlabel(\"LLM\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016453,
     "end_time": "2024-06-03T17:02:35.928279",
     "exception": false,
     "start_time": "2024-06-03T17:02:35.911826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Winning Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.112531,
     "end_time": "2024-06-03T17:02:36.056818",
     "exception": false,
     "start_time": "2024-06-03T17:02:35.944287",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ë®àÁÆó‰∏¶ÈáçÊßã counts DataFrame\n",
    "counts = (\n",
    "    df[\"class_name\"]\n",
    "    .value_counts()                      # Series: index=class_name, value=frequency\n",
    "    .rename_axis(\"Winner\")               # Êää index ÂêçÁ®±ÊîπÊàê Winner\n",
    "    .reset_index(name=\"Win Count\")       # Êää value ËÆäÊàê Win Count Ê¨Ñ‰Ωç\n",
    ")\n",
    "print(\"Columns in counts:\", counts.columns.tolist())\n",
    "\n",
    "counts_sorted = counts.sort_values(by=\"Win Count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(counts_sorted[\"Winner\"], counts_sorted[\"Win Count\"])\n",
    "\n",
    "\n",
    "for bar in bars:\n",
    "    h = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        h,\n",
    "        f\"{h}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "\n",
    "plt.title(\"Winner Distribution for Train Data\")\n",
    "plt.xlabel(\"Winner\")\n",
    "plt.ylabel(\"Win Count\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016208,
     "end_time": "2024-06-03T17:02:36.090042",
     "exception": false,
     "start_time": "2024-06-03T17:02:36.073834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üî™ | Data Split\n",
    "\n",
    "In the code snippet provided below, we will divide the existing data into training and validation using a stratification of `class_label` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.779715,
     "end_time": "2024-06-03T17:02:36.885945",
     "exception": false,
     "start_time": "2024-06-03T17:02:36.10623",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"class_label\"],\n",
    "    shuffle=True,\n",
    "    random_state=CFG.seed,    # ‚Üê Âä†‰∏äÈÄôË°åÔºå‰øùË≠âÊØèÊ¨°ÂàáÂàÜÈÉΩ‰∏ÄÊ®£\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016189,
     "end_time": "2024-06-03T17:02:36.919019",
     "exception": false,
     "start_time": "2024-06-03T17:02:36.90283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üçΩÔ∏è | Preprocessing\n",
    "\n",
    "**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n",
    "\n",
    "**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n",
    "\n",
    "Explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n",
    "- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n",
    "- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.390834,
     "end_time": "2024-06-03T17:02:40.326479",
     "exception": false,
     "start_time": "2024-06-03T17:02:36.935645",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n",
    "    preset=CFG.preset, # Name of the model\n",
    "    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016599,
     "end_time": "2024-06-03T17:02:40.360813",
     "exception": false,
     "start_time": "2024-06-03T17:02:40.344214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, let's examine what the output shape of the preprocessing layer looks like. The output shape of the layer can be represented as $(num\\_responses, sequence\\_length)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.056025,
     "end_time": "2024-06-03T17:02:41.433464",
     "exception": false,
     "start_time": "2024-06-03T17:02:40.377439",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "outs = preprocessor(df.options.iloc[0])  # Process options for the first row\n",
    "\n",
    "# Display the shape of each processed output\n",
    "for k, v in outs.items():\n",
    "    print(k, \":\", v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018502,
     "end_time": "2024-06-03T17:02:41.469384",
     "exception": false,
     "start_time": "2024-06-03T17:02:41.450882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We'll use the `preprocessing_fn` function to transform each text option using the `dataset.map(preprocessing_fn)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.024249,
     "end_time": "2024-06-03T17:02:41.510214",
     "exception": false,
     "start_time": "2024-06-03T17:02:41.485965",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_fn_train(options: list[str], class_label: int):\n",
    "    \"\"\"\n",
    "    Áî®Âú®Ë®ìÁ∑¥/È©óË≠âÔºöËº∏ÂÖ•‰∏ÄÂÄã options list ÂíåÂÆÉÁöÑ class_labelÔºå\n",
    "    ÂõûÂÇ≥ (model_inputs_dict, class_label)„ÄÇ\n",
    "    \"\"\"\n",
    "    # ÂÆâÂÖ®Ê™¢Êü•\n",
    "    if not isinstance(options, list) or len(options) == 0:\n",
    "        # full of zeros, ÂÖ©ÊÆµ input_ids, attention_mask, etc.\n",
    "        empty_inputs = {\n",
    "            k: tf.zeros((len(options) or 1, CFG.sequence_length), dtype=tf.int32)\n",
    "            for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
    "        }\n",
    "        return empty_inputs, class_label\n",
    "\n",
    "    # ÁúüÊ≠£ call preprocessor\n",
    "    model_inputs = preprocessor(options)\n",
    "    return model_inputs, class_label\n",
    "\n",
    "\n",
    "def preprocess_fn_test(options: list[str]):\n",
    "    \"\"\"\n",
    "    Áî®Âú®Ê∏¨Ë©¶/Êé®Ë´ñÔºöÂè™Êúâ optionsÔºåÂõûÂÇ≥ model_inputs_dict„ÄÇ\n",
    "    \"\"\"\n",
    "    if not isinstance(options, list) or len(options) == 0:\n",
    "        return {\n",
    "            k: tf.zeros((len(options) or 1, CFG.sequence_length), dtype=tf.int32)\n",
    "            for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
    "        }\n",
    "    return preprocessor(options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016686,
     "end_time": "2024-06-03T17:02:41.543394",
     "exception": false,
     "start_time": "2024-06-03T17:02:41.526708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üçö | DataLoader\n",
    "\n",
    "The code below sets up a robust data flow pipeline using `tf.data.Dataset` for data processing. Notable aspects of `tf.data` include its ability to simplify pipeline construction and represent components in sequences.\n",
    "\n",
    "To learn more about `tf.data`, refer to this [documentation](https://www.tensorflow.org/guide/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.026326,
     "end_time": "2024-06-03T17:02:41.586658",
     "exception": false,
     "start_time": "2024-06-03T17:02:41.560332",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(options, labels=None, batch_size=32,\n",
    "                  cache=True, shuffle_size=1024):\n",
    "    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n",
    "     # 1. Âª∫Á´ãÂéüÂßã dsÔºå‰∏¶ÈÅ∏ÊìáÂ∞çÊáâÁöÑ map function\n",
    "    if labels is not None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((options, labels))\n",
    "        map_fn = preprocess_fn_train\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(options)\n",
    "        map_fn = preprocess_fn_test\n",
    "\n",
    "    # 2. ShuffleÔºàÂè™Âú®Êúâ labels ÊôÇÔºå‰∏î shuffle_size > 0Ôºâ\n",
    "    if labels is not None and shuffle_size > 0:\n",
    "        ds = ds.shuffle(shuffle_size, seed=CFG.seed, reshuffle_each_iteration=True)\n",
    "\n",
    "    # 3. CacheÔºàÂèØÈÅ∏Ôºâ\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    # 4. Map ‚Üí preprocessing\n",
    "    ds = ds.map(map_fn, num_parallel_calls=AUTO)\n",
    "\n",
    "    # 5. Batch & Prefetch\n",
    "    ds = ds.batch(batch_size, drop_remainder=False)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016559,
     "end_time": "2024-06-03T17:02:41.619713",
     "exception": false,
     "start_time": "2024-06-03T17:02:41.603154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build Train/Valid Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "papermill": {
     "duration": 5.308478,
     "end_time": "2024-06-03T17:02:46.944786",
     "exception": false,
     "start_time": "2024-06-03T17:02:41.636308",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "train_texts = train_df[\"options\"].tolist()\n",
    "train_labels = train_df[\"class_label\"].tolist()\n",
    "train_ds = build_dataset(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    batch_size=CFG.batch_size,\n",
    "    cache=True,       # cache Ë®ìÁ∑¥ÈõÜ‰ª•Âä†ÈÄüÂ§ö epoch ÈáçË∑ë\n",
    "    shuffle_size=1024      # shuffle buffer size\n",
    ")\n",
    "\n",
    "# Valid\n",
    "valid_texts = valid_df[\"options\"].tolist()\n",
    "valid_labels = valid_df[\"class_label\"].tolist()\n",
    "valid_ds = build_dataset(\n",
    "    valid_texts,\n",
    "    valid_labels,\n",
    "    batch_size=CFG.batch_size,\n",
    "    cache=False,      # È©óË≠âÈõÜ‰∏çÈúÄË¶Å cache\n",
    "    shuffle_size=0         # ‰∏çÂÅö shuffle\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017157,
     "end_time": "2024-06-03T17:02:46.979278",
     "exception": false,
     "start_time": "2024-06-03T17:02:46.962121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ‚öì | LR Schedule\n",
    "\n",
    "Implementing a learning rate scheduler is crucial for transfer learning. The learning rate initiates at `lr_start` and gradually tapers down to `lr_min` using various techniques, including:\n",
    "- `step`: Lowering the learning rate in step-wise manner resembling stairs.\n",
    "- `cos`: Utilizing a cosine curve to gradually reduce the learning rate.\n",
    "- `exp`: Exponentially decreasing the learning rate.\n",
    "\n",
    "**Importance:** A well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.030355,
     "end_time": "2024-06-03T17:02:47.026587",
     "exception": false,
     "start_time": "2024-06-03T17:02:46.996232",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_cosine_warmup_schedule(steps_per_epoch: int):\n",
    "    total_steps   = CFG.epochs * steps_per_epoch\n",
    "    warmup_steps  = int(total_steps * CFG.warmup_ratio)\n",
    "    # 1. ÂÖàÂÅö warmupÔºö‰ªé 0 ‚Üí learning_rate\n",
    "    warmup_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=0.0,\n",
    "        decay_steps=warmup_steps,\n",
    "        end_learning_rate=CFG.learning_rate,\n",
    "        power=1.0\n",
    "    )\n",
    "    # 2. ÂÜçÂÅöÂâ©‰ΩôÊ≠•Êï∞ÁöÑ cosine decayÔºö‰ªé learning_rate ‚Üí weight_decayÔºàÂΩì‰ΩúÊúÄÂ∞è lrÔºâ\n",
    "    cosine_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=CFG.learning_rate,\n",
    "        decay_steps=total_steps - warmup_steps,\n",
    "        alpha=CFG.weight_decay/CFG.learning_rate  # Œ± = lr_min / lr_max\n",
    "    )\n",
    "    # 3. Áî® `tf.where` Êää‰∏§‰∏™ schedule ÊãºËµ∑Êù•\n",
    "    def lr_schedule_fn(step):\n",
    "        return tf.where(\n",
    "            step < warmup_steps,\n",
    "            warmup_fn(step),\n",
    "            cosine_fn(step - warmup_steps)\n",
    "        )\n",
    "    return lr_schedule_fn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.302348,
     "end_time": "2024-06-03T17:02:47.345856",
     "exception": false,
     "start_time": "2024-06-03T17:02:47.043508",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "\n",
    "steps_per_epoch = len(train_df) // CFG.batch_size\n",
    "lr_fn = get_cosine_warmup_schedule(steps_per_epoch)\n",
    "\n",
    "# ‰ΩøÁî®ÂÖßÂª∫ÁöÑ AdamW\n",
    "optimizer = AdamW(\n",
    "    learning_rate=lr_fn,\n",
    "    weight_decay=CFG.weight_decay\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017351,
     "end_time": "2024-06-03T17:02:47.381369",
     "exception": false,
     "start_time": "2024-06-03T17:02:47.364018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üíæ | Model Checkpointing\n",
    "\n",
    "The following code will create a callback that will save the best checkpoint of the model during training, which we will use for inference in the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.025064,
     "end_time": "2024-06-03T17:02:47.424145",
     "exception": false,
     "start_time": "2024-06-03T17:02:47.399081",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Êää checkpoint Â≠òÂà∞Â∞àÊ°àÁöÑ checkpoints ÁõÆÈåÑÔºåÂÖàÁ¢∫‰øùÂÆÉÂ≠òÂú®\n",
    "ckpt_dir = Path(\"checkpoints\")\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(ckpt_dir / \"best_model.{epoch:02d}-{val_loss:.4f}.weights.h5\"),\n",
    "    monitor=\"val_loss\",             # Ë¶ÅÂ∞çÊáâ compile ÊôÇÁöÑ loss ÂêçÁ®±\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,          # Âè™Â≠ò weights\n",
    "    mode=\"min\",                      # loss Ë∂ä‰Ωé‰ª£Ë°®Ë∂äÂ•Ω\n",
    "    verbose=1                        # ÂÑ≤Â≠òÊôÇÈ°ØÁ§∫Ë®äÊÅØ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017265,
     "end_time": "2024-06-03T17:02:47.553968",
     "exception": false,
     "start_time": "2024-06-03T17:02:47.536703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ü§ñ | Modeling\n",
    "\n",
    "The `KerasNLP` library provides various NLP model architectures such as `Bert`, `Roberta`, `DebertaV3`, and more. While this notebook focuses on `DebertaV3`, you can explore others in the [KerasNLP documentation](https://keras.io/api/keras_nlp/models/). For a deeper understanding, refer to the [getting started guide](https://keras.io/guides/keras_nlp/getting_started/).\n",
    "\n",
    "Our approach utilizes `keras_nlp.models.DebertaV3Classifier` to process each prompt and response pair, generating output embeddings. We then concatenate these embeddings and pass them through a Pooling layer and a classifier to obtain logits, followed by a `softmax` function for the final output.\n",
    "\n",
    "When dealing with multiple responses, we use a weight-sharing strategy. This means we provide the model with one response at a time along with the prompt `(P + R_A)`, `(P + R_B)`, etc., using the same model weights for all responses. After obtaining embeddings for all responses, we concatenate them and apply average pooling. Next, we use a `Linear/Dense` layer along with the `Softmax` function as the classifier for the final result. Providing all responses at once would increase text length and complicate model handling. Note that, in the classifier, we use 3 classes for `winner_model_a`, `winner_model_b`, and `draw` cases.\n",
    "\n",
    "The diagram below illustrates this approach:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://i.postimg.cc/g0gcvy3f/Kaggle-drawio.png\">\n",
    "</div>\n",
    "\n",
    "From a coding perspective, note that we use the same model for all responses with shared weights, contrary to the separate models implied in the diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 8.231591,
     "end_time": "2024-06-03T17:02:55.802843",
     "exception": false,
     "start_time": "2024-06-03T17:02:47.571252",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define input layers\n",
    "\n",
    "import math\n",
    "from tensorflow.keras import mixed_precision, regularizers\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "# 1. Mixed‚Äêprecision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# 2. Define inputs matching preprocessor output keys\n",
    "inputs = {\n",
    "    \"input_ids\": keras.Input(\n",
    "        shape=(2, CFG.sequence_length), dtype=tf.int32, name=\"input_ids\"\n",
    "    ),\n",
    "    \"attention_mask\": keras.Input(\n",
    "        shape=(2, CFG.sequence_length), dtype=tf.int32, name=\"attention_mask\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# 3. Backbone + freeze; expects 'token_ids' & 'padding_mask'\n",
    "backbone = keras_nlp.models.DebertaV3Backbone.from_preset(CFG.preset)\n",
    "backbone.trainable = False\n",
    "\n",
    "# 4. Slice & rename inputs for backbone\n",
    "def slice_pair_and_rename(inputs_dict, idx):\n",
    "    return {\n",
    "        \"token_ids\":    inputs_dict[\"input_ids\"][:, idx, :],\n",
    "        \"padding_mask\": inputs_dict[\"attention_mask\"][:, idx, :],\n",
    "    }\n",
    "# Backbone (frozen initially)\n",
    "backbone = keras_nlp.models.DebertaV3Backbone.from_preset(CFG.preset)\n",
    "backbone.trainable = False\n",
    "\n",
    "# Helper to slice the pair dimension and rename keys for the backbone\n",
    "def slice_pair_and_rename(inputs_dict, idx):\n",
    "    return {\n",
    "        \"token_ids\":    inputs_dict[\"input_ids\"][:, idx, :],\n",
    "        \"padding_mask\": inputs_dict[\"attention_mask\"][:, idx, :],\n",
    "    }\n",
    "embed_a = backbone(slice_pair_and_rename(inputs, 0))\n",
    "embed_b = backbone(slice_pair_and_rename(inputs, 1))\n",
    "\n",
    "# 5. Head\n",
    "x = keras.layers.Concatenate(axis=1)([embed_a, embed_b])\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = keras.layers.Dropout(CFG.dropout_rate)(x)\n",
    "outputs = keras.layers.Dense(\n",
    "    len(CFG.class_names),\n",
    "    activation=\"softmax\",\n",
    "    dtype=\"float32\",\n",
    "    kernel_regularizer=regularizers.l2(CFG.weight_decay),\n",
    "    name=\"classifier\",\n",
    ")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# --- 6. Custom Cosine‚Äêwith‚ÄêWarmup Schedule ---\n",
    "class CosineWarmupSchedule(LearningRateSchedule):\n",
    "    def __init__(self, lr_max, lr_min, warmup_steps, total_steps):\n",
    "        super().__init__()\n",
    "        self.lr_max = lr_max\n",
    "        self.lr_min = lr_min\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # convert to float tensors\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, tf.float32)\n",
    "\n",
    "        # linear warmup: 0 -> lr_max\n",
    "        warmup_lr = self.lr_max * (step / warmup_steps)\n",
    "\n",
    "        # cosine decay: lr_max -> lr_min\n",
    "        decay_step = step - warmup_steps\n",
    "        decay_steps = total_steps - warmup_steps\n",
    "        cosine_decay = 0.5 * (1 + tf.cos(math.pi * decay_step / decay_steps))\n",
    "        decay_lr = self.lr_min + (self.lr_max - self.lr_min) * cosine_decay\n",
    "\n",
    "        return tf.where(step < warmup_steps, warmup_lr, decay_lr)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"lr_max\": self.lr_max,\n",
    "            \"lr_min\": self.lr_min,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"total_steps\": self.total_steps,\n",
    "        }\n",
    "\n",
    "# compute schedule parameters\n",
    "steps_per_epoch = len(train_df) // CFG.batch_size\n",
    "total_steps  = CFG.head_only_epochs * steps_per_epoch + CFG.fine_tune_epochs * steps_per_epoch\n",
    "warmup_steps = int(CFG.head_only_epochs * steps_per_epoch * CFG.warmup_ratio)\n",
    "\n",
    "lr_schedule = CosineWarmupSchedule(\n",
    "    lr_max=CFG.learning_rate,\n",
    "    lr_min=CFG.weight_decay,\n",
    "    warmup_steps=warmup_steps,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "# --- 7. Optimizer & Compile ---\n",
    "# Stage 1: train head only\n",
    "optimizer_stage1 = AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay=CFG.weight_decay,\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=optimizer_stage1,\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    ")\n",
    "print(f\" Stage1: training head for {CFG.head_only_epochs} epochs\")\n",
    "history_head = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=CFG.head_only_epochs,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=CFG.head_only_epochs,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(ckpt_dir / \"best_model_head_only.weights.h5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Stage 2: unfreeze and fine‚Äêtune\n",
    "backbone.trainable = True\n",
    "for layer in backbone.layers[:-CFG.unfrozen_backbone_layers]:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer_stage2 = AdamW(\n",
    "    learning_rate=CFG.learning_rate * 0.1,\n",
    "    weight_decay=CFG.weight_decay,\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=optimizer_stage2,\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    ")\n",
    "print(f\" Stage2: fine‚Äêtuning last {CFG.unfrozen_backbone_layers} layers for {CFG.fine_tune_epochs} epochs\")\n",
    "history_ft = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    initial_epoch=CFG.head_only_epochs,\n",
    "    epochs=CFG.head_only_epochs + CFG.fine_tune_epochs,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=CFG.fine_tune_epochs,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(\n",
    "                ckpt_dir / \"best_model_finetuned.epoch{epoch:02d}_val{val_loss:.4f}.weights.h5\"\n",
    "            ),\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018012,
     "end_time": "2024-06-03T17:02:55.83988",
     "exception": false,
     "start_time": "2024-06-03T17:02:55.821868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.051632,
     "end_time": "2024-06-03T17:02:55.909391",
     "exception": false,
     "start_time": "2024-06-03T17:02:55.857759",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018914,
     "end_time": "2024-06-03T17:02:55.947016",
     "exception": false,
     "start_time": "2024-06-03T17:02:55.928102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Plot\n",
    "\n",
    "In the model graph below, it may seem there are **four** inputs, but actually, there are **two** as discussed before. Our input consists of two parts, one for each response. However, for each input, we have `token_ids` and `padding_mask`, which makes it look like we have four inputs, but in reality, we have two inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.025569,
     "end_time": "2024-06-03T17:02:55.991355",
     "exception": false,
     "start_time": "2024-06-03T17:02:55.965786",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. ÂæûÂÖ©ÂÄã history Â∞çË±°Ë£°Âêà‰ΩµÊåáÊ®ô\n",
    "loss = history_head.history['loss'] + history_ft.history['loss']\n",
    "val_loss = history_head.history.get('val_loss', []) + history_ft.history.get('val_loss', [])\n",
    "\n",
    "acc = history_head.history.get('sparse_categorical_accuracy', history_head.history.get('accuracy', [])) \\\n",
    "      + history_ft.history.get('sparse_categorical_accuracy', history_ft.history.get('accuracy', []))\n",
    "val_acc = history_head.history.get('val_sparse_categorical_accuracy', history_head.history.get('val_accuracy', [])) \\\n",
    "          + history_ft.history.get('val_sparse_categorical_accuracy', history_ft.history.get('val_accuracy', []))\n",
    "\n",
    "# 2. x Ëª∏ÔºöÁ∏ΩÂÖ±ÁöÑ epoch \n",
    "epochs = list(range(1, len(loss) + 1))\n",
    "\n",
    "# 3. Áπ™Âúñ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Â∑¶ÔºöLoss\n",
    "axes[0].plot(epochs, loss, label='Train Loss')\n",
    "if val_loss:\n",
    "    axes[0].plot(epochs, val_loss, label='Validation Loss')\n",
    "axes[0].axvline(x=CFG.head_only_epochs + 0.5, color='gray', linestyle='--',\n",
    "                label='Unfreeze Backbone')  # Ê†áÂá∫Ëß£ÂÜªÁÇπ\n",
    "axes[0].set_title('Loss over Epochs')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Âè≥ÔºöAccuracy\n",
    "axes[1].plot(epochs, acc, label='Train Accuracy')\n",
    "if val_acc:\n",
    "    axes[1].plot(epochs, val_acc, label='Validation Accuracy')\n",
    "axes[1].axvline(x=CFG.head_only_epochs + 0.5, color='gray', linestyle='--',\n",
    "                label='Unfreeze Backbone')\n",
    "axes[1].set_title('Accuracy over Epochs')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.805618,
     "end_time": "2024-06-03T19:18:25.735457",
     "exception": false,
     "start_time": "2024-06-03T19:18:24.929839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Best Model\n",
    "\n",
    "After training, let's load the weight with best result to get the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.546342,
     "end_time": "2024-06-03T19:18:29.025014",
     "exception": false,
     "start_time": "2024-06-03T19:18:26.478672",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. ÊåáÂÆö checkpoint ÁõÆÈåÑ\n",
    "ckpt_dir = Path(\"checkpoints\")\n",
    "if not ckpt_dir.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint directory not found: {ckpt_dir}\")\n",
    "\n",
    "# 2. ÂàóÂá∫ÊâÄÊúâ .weights.h5 Êñá‰ª∂ÔºàÂåÖÊã¨ head-only „ÄÅ finetunedÔºâ\n",
    "weight_files = list(ckpt_dir.glob(\"*.weights.h5\"))\n",
    "if not weight_files:\n",
    "    raise FileNotFoundError(f\"No .weights.h5 files found in {ckpt_dir}\")\n",
    "\n",
    "# 3. ÂæûÊñá‰ª∂Âêç‰∏≠Ëß£ÊûêÂá∫ epoch Êï∏Â≠óÔºõhead-only Êñá‰ª∂Ë®≠ÁÇ∫ 0\n",
    "def extract_epoch(fp: Path) -> int:\n",
    "    m = re.search(r\"epoch(\\d+)\", fp.name)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "# 4. ÈÅ∏Âá∫ epoch ÊúÄÂ§ßÁöÑÊñá‰ª∂\n",
    "latest_file = max(weight_files, key=extract_epoch)\n",
    "\n",
    "# 5. Âä†ËºâÊ¨äÈáç\n",
    "model.load_weights(str(latest_file))\n",
    "print(f\"Loaded weights from: {latest_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.754041,
     "end_time": "2024-06-03T19:18:30.53816",
     "exception": false,
     "start_time": "2024-06-03T19:18:29.784119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üß™ | Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.156834,
     "end_time": "2024-06-03T19:18:32.520369",
     "exception": false,
     "start_time": "2024-06-03T19:18:31.363535",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build test dataset\n",
    "test_texts = test_df[\"options\"].tolist()\n",
    "\n",
    "test_ds = build_dataset(\n",
    "    options=test_texts,\n",
    "    labels=None,                     # ÊòéÁ°ÆÂëäËØâÂÆÉÊ≤°Êúâ labels\n",
    "    batch_size=CFG.batch_size,       # ÊàñËÄÖ min(len(test_df), CFG.batch_size)\n",
    "    cache=False,                     # ‰∏ç cache\n",
    "    shuffle_size=0                   # ‰∏ç shuffle\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 6.510153,
     "end_time": "2024-06-03T19:18:39.767367",
     "exception": false,
     "start_time": "2024-06-03T19:18:33.257214",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Make predictions using the trained model on test data\n",
    "test_preds = model.predict(test_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.768879,
     "end_time": "2024-06-03T19:18:41.329256",
     "exception": false,
     "start_time": "2024-06-03T19:18:40.560377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üì¨ | Submission\n",
    "\n",
    "Following code will prepare the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.751809,
     "end_time": "2024-06-03T19:18:42.839827",
     "exception": false,
     "start_time": "2024-06-03T19:18:42.088018",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. ÊääÈ†êÊ∏¨ÁµêÊûúÂÅöÊàê DataFrame\n",
    "preds_df = pd.DataFrame(test_preds, columns=CFG.class_names)\n",
    "\n",
    "# 2. Âêà‰Ωµ id\n",
    "sub_df = pd.concat([test_df[\"id\"].reset_index(drop=True), preds_df], axis=1)\n",
    "\n",
    "# 3. ÔºàÂèØÈÅ∏ÔºâÂä†ÊúÄÁµÇÈ†êÊ∏¨È°ûÂà•\n",
    "sub_df[\"pred_class\"] = preds_df.idxmax(axis=1)\n",
    "\n",
    "# 4. ÂØ´Ê™î‰∏¶Ëº∏Âá∫Á¢∫Ë™ç\n",
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv with columns:\", sub_df.columns.tolist())\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.740084,
     "end_time": "2024-06-03T19:18:44.386408",
     "exception": false,
     "start_time": "2024-06-03T19:18:43.646324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üî≠ | Future Directions\n",
    "\n",
    "In this notebook, we've achieved a good score with a small model and modest token length. But there's plenty of room to improve. Here's how:\n",
    "\n",
    "1. Try bigger models like `Deberta-Base` or `Deberta-Small`, or even LLMs like `Gemma`.\n",
    "2. Increase max token length to reduce loss of data.\n",
    "3. Use a five-fold cross-validation and ensemble to make the model robust and get better scores.\n",
    "4. Add augmentation like shuffling response orders for more robust performance.\n",
    "5. Train for more epochs.\n",
    "6. Tune the learning rate scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.830152,
     "end_time": "2024-06-03T19:18:45.950588",
     "exception": false,
     "start_time": "2024-06-03T19:18:45.120436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üìå | Reference\n",
    "\n",
    "* [LLM Science Exam: KerasCore + KerasNLP [TPU]](https://www.kaggle.com/code/awsaf49/llm-science-exam-kerascore-kerasnlp-tpu)\n",
    "* [AES 2.0: KerasNLP Starter](https://www.kaggle.com/code/awsaf49/aes-2-0-kerasnlp-starter)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "sourceId": 86518,
     "sourceType": "competition"
    },
    {
     "modelId": 2820,
     "modelInstanceId": 4684,
     "sourceId": 6063,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8265.260972,
   "end_time": "2024-06-03T19:18:50.009067",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-03T17:01:04.748095",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
