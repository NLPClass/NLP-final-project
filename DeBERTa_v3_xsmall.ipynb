{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 0. 單卡環境 & 套件\n",
        "# ===============================================================\n",
        "import os, torch, gc, warnings\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"NCCL_P2P_DISABLE\"]    = \"1\"\n",
        "torch.cuda.set_device(0)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip -q install \"transformers>=4.41.2\" \"accelerate>=0.27.2\" peft datasets sentencepiece\n",
        "\n",
        "# ===============================================================\n",
        "# 1. 讀資料\n",
        "# ===============================================================\n",
        "import pandas as pd, numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "train_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n",
        "test_df  = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n",
        "\n",
        "train_df[\"label\"] = (\n",
        "      train_df[\"winner_model_a\"]*0 +\n",
        "      train_df[\"winner_model_b\"]*1 +\n",
        "      train_df[\"winner_tie\"]    *2\n",
        ").astype(int)\n",
        "train_df = train_df[[\"prompt\",\"response_a\",\"response_b\",\"label\"]]\n",
        "\n",
        "# ===============================================================\n",
        "# 2. Tokenizer & Arrow Dataset\n",
        "# ===============================================================\n",
        "from transformers import AutoTokenizer\n",
        "MODEL_PATH = \"/kaggle/input/deberta-xs/transformers/default/1/deberta-xs\"   # ← 改成你的資料夾根\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
        "MAX_LEN = 512                                            # 提升訊息覆蓋\n",
        "\n",
        "def tok_fn(batch):\n",
        "    text = [p + tok.sep_token + ra + tok.sep_token + rb\n",
        "            for p,ra,rb in zip(batch[\"prompt\"],\n",
        "                               batch[\"response_a\"],\n",
        "                               batch[\"response_b\"])]\n",
        "    return tok(text, truncation=True, max_length=MAX_LEN, padding=\"max_length\")\n",
        "\n",
        "ds = Dataset.from_pandas(train_df).map(tok_fn, batched=True, num_proc=4)\n",
        "ds = ds.train_test_split(test_size=0.05, seed=42)\n",
        "for sp in (\"train\",\"test\"):\n",
        "    ds[sp].set_format(\"torch\", [\"input_ids\",\"attention_mask\",\"label\"])\n",
        "\n",
        "# ===============================================================\n",
        "# 3. DeBERTa-xsmall + LoRA (FP32)\n",
        "# ===============================================================\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "base = AutoModelForSequenceClassification.from_pretrained(\n",
        "           MODEL_PATH, num_labels=3, local_files_only=True).to(\"cuda\")\n",
        "base.gradient_checkpointing_enable()\n",
        "\n",
        "model = get_peft_model(base, LoraConfig(\n",
        "           r=8, lora_alpha=16,\n",
        "           target_modules=[\"query_proj\",\"value_proj\"],\n",
        "           lora_dropout=0.05, bias=\"none\",\n",
        "           task_type=\"SEQ_CLS\"))\n",
        "\n",
        "# ===============================================================\n",
        "# 4. Trainer\n",
        "# ===============================================================\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir          = \"deberta-xs-lora\",\n",
        "    per_device_train_batch_size = 1,\n",
        "    per_device_eval_batch_size  = 1,\n",
        "    gradient_accumulation_steps = 16,   # batch=16 等效\n",
        "    learning_rate       = 1e-4,\n",
        "    num_train_epochs    = 4,\n",
        "    max_grad_norm       = 0.0,          # clip 關掉 (避免 fp16 unscale)\n",
        "    logging_steps       = 20,\n",
        "    eval_strategy = \"no\",\n",
        "    save_strategy       = \"no\",\n",
        "    report_to           = \"none\"\n",
        ")\n",
        "\n",
        "metric = lambda p: {\"acc\": (p.predictions.argmax(1) == p.label_ids).mean()}\n",
        "trainer = Trainer(model,args,\n",
        "                  train_dataset=ds[\"train\"],\n",
        "                  eval_dataset =ds[\"test\"],\n",
        "                  compute_metrics=metric)\n",
        "trainer.train()\n",
        "\n",
        "# ===============================================================\n",
        "# 5. 推論 — DeBERTa (N,3)\n",
        "# ===============================================================\n",
        "def deberta_logits(df):\n",
        "    tmp = Dataset.from_pandas(df[[\"prompt\",\"response_a\",\"response_b\"]])\\\n",
        "          .map(tok_fn, batched=True)\n",
        "    tmp.set_format(\"torch\", [\"input_ids\",\"attention_mask\"])\n",
        "    pred = trainer.predict(tmp).predictions\n",
        "    return torch.softmax(torch.tensor(pred), -1).cpu().numpy()\n",
        "\n",
        "disc_probs = deberta_logits(test_df)           # (N,3)\n",
        "\n",
        "# ===============================================================\n",
        "# 6. 推論 — PairRM (N,2)\n",
        "# ===============================================================\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "RM_PATH = \"/kaggle/input/pairrm-0.4b/transformers/default/1/pairrm-0.4b\"\n",
        "rm_tok = AutoTokenizer.from_pretrained(RM_PATH, local_files_only=True)\n",
        "rm     = AutoModelForSequenceClassification.from_pretrained(\n",
        "            RM_PATH, local_files_only=True, device_map=\"auto\")\n",
        "\n",
        "def rm_pair_prob(p,a,b):\n",
        "    txt = f\"{p} {rm_tok.eos_token}{a}{rm_tok.eos_token}{b}\"\n",
        "    with torch.no_grad():\n",
        "        out = rm(**rm_tok(txt, return_tensors=\"pt\",\n",
        "                          truncation=True, max_length=2048).to(rm.device)).logits\n",
        "    return torch.softmax(out.squeeze(), -1).cpu().numpy()\n",
        "\n",
        "rm_probs = np.vstack([rm_pair_prob(p,a,b) for p,a,b in zip(\n",
        "    test_df.prompt, test_df.response_a, test_df.response_b)])     # (N,2)\n",
        "\n",
        "# ===============================================================\n",
        "# 7. Ensemble (連續 Tie 機率)\n",
        "# ===============================================================\n",
        "alpha = 0.55\n",
        "prob        = disc_probs.copy()                       # (N,3)\n",
        "prob[:, :2] = alpha*disc_probs[:, :2] + (1-alpha)*rm_probs\n",
        "prob        = prob / prob.sum(axis=1, keepdims=True)  # Normalize\n",
        "\n",
        "# ===============================================================\n",
        "# 8. 輸出 submission.csv\n",
        "# ===============================================================\n",
        "sub = pd.DataFrame({\n",
        "    \"id\":             test_df.id,\n",
        "    \"winner_model_a\": prob[:,0],\n",
        "    \"winner_model_b\": prob[:,1],\n",
        "    \"winner_tie\":     prob[:,2]\n",
        "})\n",
        "sub.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
        "print(\"✅ submission.csv 生成完成！\")"
      ],
      "metadata": {
        "id": "DtxPAxCABlT0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}