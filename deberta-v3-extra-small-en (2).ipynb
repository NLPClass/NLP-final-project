{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":6063,"sourceType":"modelInstanceVersion","modelInstanceId":4684,"modelId":2820}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":8265.260972,"end_time":"2024-06-03T19:18:50.009067","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-03T17:01:04.748095","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\nThis starter notebook is provided by the Keras team.</center>","metadata":{"papermill":{"duration":0.014876,"end_time":"2024-06-03T17:01:07.523093","exception":false,"start_time":"2024-06-03T17:01:07.508217","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Getting Started on LLM Classification Fine Tuning with [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras)\n\n<div align=\"center\">\n    <img src=\"https://i.ibb.co/wJMF5HL/lmsys.png\">\n</div>\n\nIn this competition, our aim is to predict which LLM responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs). In other words, the goal of the competition is to predict the preferences of the judges and determine the likelihood that a given prompt/response pair is selected as the winner. This notebook will guide you through the process of fine-tuning the **DebertaV3** model for this competition using the **Shared Weight** strategy with KerasNLP. This strategy is similar to how Multiple Choice Question (MCQ) models are trained. Additionally, we will use mixed precision for faster training and inference.\n\n**Did you know**: This notebook is backend-agnostic, which means it supports TensorFlow, PyTorch, and JAX backends. However, the best performance can be achieved with `JAX`. KerasNLP and Keras enable the choice of the preferred backend. Explore further details on [Keras](https://keras.io/keras_3/).\n\n**Note**: For a deeper understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/).\n","metadata":{"papermill":{"duration":0.014071,"end_time":"2024-06-03T17:01:07.551303","exception":false,"start_time":"2024-06-03T17:01:07.537232","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 📚 | Import Libraries ","metadata":{"papermill":{"duration":0.014736,"end_time":"2024-06-03T17:01:07.580011","exception":false,"start_time":"2024-06-03T17:01:07.565275","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\n\nimport keras_nlp\nimport keras\nimport tensorflow as tf\n#import tensorflow_addons as tfa\n\nimport random\nimport numpy as np \nimport pandas as pd\nfrom tqdm import tqdm\nimport json\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport plotly.express as px\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"_kg_hide-output":true,"papermill":{"duration":14.715162,"end_time":"2024-06-03T17:01:22.309231","exception":false,"start_time":"2024-06-03T17:01:07.594069","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\n\n# 將這行放在程式最前面，替換 YOUR_API_KEY 為你的實際 key\nkey=\"cfbd7f24413a77f911641acae8914b1dc77e1b83\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install --upgrade wandb  # 確保使用最新版本\n#from wandb.keras import WandbCallback\n\n#wandb.login(key=\"cfbd7f24413a77f911641acae8914b1dc77e1b83\")\n#wandb.init(project=\"kerasnlp-training\", name=\"your-run-name\")  # 自訂專案與實驗名稱","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Library Version","metadata":{"papermill":{"duration":0.014604,"end_time":"2024-06-03T17:01:22.338854","exception":false,"start_time":"2024-06-03T17:01:22.32425","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# ⚙️ | Configuration","metadata":{"papermill":{"duration":0.014156,"end_time":"2024-06-03T17:01:22.405533","exception":false,"start_time":"2024-06-03T17:01:22.391377","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    seed = 42  # Random seed\n\n    # 模型和输入\n    preset = \"deberta_v3_extra_small_en\"\n    sequence_length = 512\n\n    # 两阶段总共跑的 epoch = head_only_epochs + fine_tune_epochs\n    head_only_epochs = 3        # 前 3 轮只训 head\n    fine_tune_epochs   = 5      # 接着 5 轮微调 backbone\n    # （你也可以把 epochs = head_only_epochs + fine_tune_epochs = 8，删掉 epochs 字段）\n\n    # 数据和批次\n    batch_size = 16\n\n    # 优化器&调度\n    learning_rate = 5e-6\n    weight_decay  = 1e-4\n    # Warmup 比例调大些，让最开始的几步能更稳定地升到 lr_max\n    warmup_ratio = 0.2\n\n    # 正则化\n    # Head 里多加一层 Dense+Dropout，所以 DropoutRate 调高到 0.3 让它更抗过拟合\n    dropout_rate = 0.3\n\n    # 微调策略：解冻 backbone 最后多少层\n    # 之前是 4 层，建议解冻更多层试试，比如 12\n    unfrozen_backbone_layers = 12\n\n    # 标签映射\n    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}\n    name2label = {v:k for k, v in label2name.items()}\n    class_labels = list(label2name.keys())\n    class_names  = list(label2name.values())","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.023008,"end_time":"2024-06-03T17:01:22.442868","exception":false,"start_time":"2024-06-03T17:01:22.41986","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ♻️ | Reproducibility \nSets value for random seed to produce similar result in each run.","metadata":{"papermill":{"duration":0.014043,"end_time":"2024-06-03T17:01:22.471173","exception":false,"start_time":"2024-06-03T17:01:22.45713","status":"completed"},"tags":[]}},{"cell_type":"code","source":"tf.keras.utils.set_random_seed(CFG.seed)\nnp.random.seed(CFG.seed)\nrandom.seed(CFG.seed)","metadata":{"papermill":{"duration":0.021526,"end_time":"2024-06-03T17:01:22.506961","exception":false,"start_time":"2024-06-03T17:01:22.485435","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🧮 | Mixed Precision\n\nIn this notebook, we will use mixed precision instead of float32 precision for training and inference to reduce GPU memory usage. This will ultimately allow us to use larger batch sizes, thus reducing our training and inference time.","metadata":{"papermill":{"duration":0.01407,"end_time":"2024-06-03T17:01:22.535249","exception":false,"start_time":"2024-06-03T17:01:22.521179","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tensorflow.keras import mixed_precision\nmixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"papermill":{"duration":0.020673,"end_time":"2024-06-03T17:01:22.570123","exception":false,"start_time":"2024-06-03T17:01:22.54945","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📁 | Dataset Path ","metadata":{"papermill":{"duration":0.013868,"end_time":"2024-06-03T17:01:22.598204","exception":false,"start_time":"2024-06-03T17:01:22.584336","status":"completed"},"tags":[]}},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/llm-classification-finetuning'","metadata":{"papermill":{"duration":0.020401,"end_time":"2024-06-03T17:01:22.63286","exception":false,"start_time":"2024-06-03T17:01:22.612459","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📖 | Meta Data \n\nThe competition dataset comprises user interactions from the ChatBot Arena. In each interaction, a judge presents one or more prompts to two different large language models and then indicates which model provided the more satisfactory response. The training data contains `55,000` rows, with an expected `25,000` rows in the test set.\n\n## Files\n\n### `train.csv`\n- `id`: Unique identifier for each row.\n- `model_[a/b]`: Model identity, present in train.csv but not in test.csv.\n- `prompt`: Input prompt given to both models.\n- `response_[a/b]`: Model_[a/b]'s response to the prompt.\n- `winner_model_[a/b/tie]`: Binary columns indicating the judge's selection (ground truth target).\n\n### `test.csv`\n- `id`: Unique identifier for each row.\n- `prompt`: Input prompt given to both models.\n- `response_[a/b]`: Model_[a/b]'s response to the prompt.\n\n> Note that each interaction may have multiple prompts and responses, but this notebook will use only **one prompt per interaction**. You can choose to use all prompts and responses. Additionally, prompts and responses in the dataframe are provided as string-formatted lists, so they need to be converted to literal lists using `eval()`.\n","metadata":{"papermill":{"duration":0.01414,"end_time":"2024-06-03T17:01:22.661285","exception":false,"start_time":"2024-06-03T17:01:22.647145","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Train Data","metadata":{"papermill":{"duration":0.013838,"end_time":"2024-06-03T17:01:22.689785","exception":false,"start_time":"2024-06-03T17:01:22.675947","status":"completed"},"tags":[]}},{"cell_type":"code","source":"''''# Load Train Data\ndf = pd.read_csv(f'{BASE_PATH}/train.csv') \n\n# Sample data\n# df = df.sample(frac=0.10)\n\n# Take the first prompt and its associated response\ndf[\"prompt\"] = df.prompt.map(lambda x: eval(x)[0])\ndf[\"response_a\"] = df.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\ndf[\"response_b\"] = df.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])\n\n# Label conversion\ndf[\"class_name\"] = df[[\"winner_model_a\", \"winner_model_b\" , \"winner_tie\"]].idxmax(axis=1)\ndf[\"class_label\"] = df.class_name.map(CFG.name2label)\n\n# Show Sample\ndf.head()\n'''\nimport json\nfrom pathlib import Path\n\n# 1. 建立路徑\nBASE_PATH = Path(BASE_PATH)  # 如果你之前已經是 str，這行可把它轉成 Path\ntrain_file = BASE_PATH / \"train.csv\"\nif not train_file.exists():\n    raise FileNotFoundError(f\"找不到訓練檔案：{train_file}\")\n\n# 2. 讀檔並打散\ndf = pd.read_csv(train_file)\ndf = df.sample(frac=1, random_state=CFG.seed).reset_index(drop=True)\n\n# 3. 安全地解析 JSON string 並取第一個元素\ndef safe_first_element(json_str: str):\n    if not isinstance(json_str, str) or len(json_str) == 0:\n        return \"\"\n    # 把 'null' 換成 JSON 可接受的空字串\n    cleaned = json_str.replace(\"null\", '\"\"')\n    try:\n        arr = json.loads(cleaned)\n        if isinstance(arr, list) and len(arr) > 0:\n            return arr[0]\n    except json.JSONDecodeError:\n        pass\n    return \"\"\n\ndf[\"prompt\"]     = df[\"prompt\"].apply(safe_first_element)\ndf[\"response_a\"] = df[\"response_a\"].apply(safe_first_element)\ndf[\"response_b\"] = df[\"response_b\"].apply(safe_first_element)\n\n# 4. 標籤轉換（和你原本一樣）\ndf[\"class_name\"]  = df[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].idxmax(axis=1)\ndf[\"class_label\"] = df[\"class_name\"].map(CFG.name2label)\n\n# 5. 檢視結果\ndf.head()\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":8.625467,"end_time":"2024-06-03T17:01:31.32943","exception":false,"start_time":"2024-06-03T17:01:22.703963","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"''''# 檢查缺失值\nmissing_values = df.isnull().sum()\nprint(\"缺失值統計：\")\nprint(missing_values)\n\n# 如果你只想列出有缺失值的欄位\nmissing_columns = missing_values[missing_values > 0]\nprint(\"\\n有缺失值的欄位：\")\nprint(missing_columns)\n'''\n# 1. 計算缺失值\nmissing_values = df.isnull().sum()\nmissing_columns = missing_values[missing_values > 0]\n\nprint(\"缺失值統計：\")\nprint(missing_values)\n\nif not missing_columns.empty:\n    print(\"\\n有缺失值的欄位：\")\n    print(missing_columns)\n\n# 2. 互動式顯示缺失值表格（可排序／篩選）\n#from ace_tools import display_dataframe_to_user\n\nmissing_df = (\n    missing_values\n    .rename(\"missing_count\")\n    .to_frame()\n    .assign(missing_pct=lambda d: d[\"missing_count\"] / len(df) * 100)\n    .reset_index()\n    .rename(columns={\"index\": \"column\"})\n    .query(\"missing_count > 0\")\n)\n\n#display_dataframe_to_user(\"缺失值欄位一覽\", missing_df)\nprint(\"\\n缺失值欄位一覽：\")\nprint(missing_df)\n\n# 3. 處理缺失值\n#    (a) 針對關鍵欄位直接丟棄含 NaN 的列\ndf = df.dropna(subset=[\"prompt\", \"response_a\", \"response_b\"]).reset_index(drop=True)\n\n#    (b) 其餘欄位用空字串填補\ndf = df.fillna(\"\")\n\n# 4. 最後檢視處理後的前幾筆\nprint(df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test Data","metadata":{"papermill":{"duration":0.014609,"end_time":"2024-06-03T17:01:31.359794","exception":false,"start_time":"2024-06-03T17:01:31.345185","status":"completed"},"tags":[]}},{"cell_type":"code","source":"'''# Load Test Data\ntest_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n\n# Take the first prompt and response\ntest_df[\"prompt\"] = test_df.prompt.map(lambda x: eval(x)[0])\ntest_df[\"response_a\"] = test_df.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\ntest_df[\"response_b\"] = test_df.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])\n\n# Show Sample\ntest_df.head()\n'''\nimport json\nfrom pathlib import Path\n\n# 1. 確保 BASE_PATH 是 Path 物件\nBASE_PATH = Path(BASE_PATH)\ntest_file = BASE_PATH / \"test.csv\"\nif not test_file.exists():\n    raise FileNotFoundError(f\"找不到測試檔案：{test_file}\")\n\n# 2. 讀取測試資料\ntest_df = pd.read_csv(test_file)\n\n# 3. 安全解析函式（同訓練集）\ndef safe_first_element(json_str: str):\n    if not isinstance(json_str, str) or len(json_str) == 0:\n        return \"\"\n    cleaned = json_str.replace(\"null\", '\"\"')\n    try:\n        arr = json.loads(cleaned)\n        if isinstance(arr, list) and len(arr) > 0:\n            return arr[0]\n    except json.JSONDecodeError:\n        pass\n    return \"\"\n\n# 4. 處理 prompt / response 欄位\ntest_df[\"prompt\"]     = test_df[\"prompt\"].apply(safe_first_element)\ntest_df[\"response_a\"] = test_df[\"response_a\"].apply(safe_first_element)\ntest_df[\"response_b\"] = test_df[\"response_b\"].apply(safe_first_element)\n\n# 5. 檢視前幾筆\nprint(test_df.head())\n","metadata":{"papermill":{"duration":0.034633,"end_time":"2024-06-03T17:01:31.409034","exception":false,"start_time":"2024-06-03T17:01:31.374401","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Alec 查看資料狀態","metadata":{}},{"cell_type":"code","source":"# 1. 計算 winner\nwinner = df[['winner_model_a','winner_model_b','winner_tie']].idxmax(axis=1).map({\n    'winner_model_a':'A_win',\n    'winner_model_b':'B_win',\n    'winner_tie':'Tie'\n})\n# 2. 計算比例並排序\ncounts = winner.value_counts(normalize=True).sort_index()  \n\n# 3. 繪圖\nimport matplotlib.pyplot as plt\n\nplt.figure()\nax = counts.plot.bar(title='Winner Distribution')\nax.set_ylabel('Proportion')\n\n# 4. 標註百分比\nfor patch in ax.patches:\n    height = patch.get_height()\n    ax.annotate(f\"{height:.1%}\",\n                (patch.get_x() + patch.get_width() / 2, height),\n                ha='center', va='bottom')\n\nplt.show()\n\n# 5. 印出數值\nprint(counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pair_stats = (\n    df\n    .groupby(['model_a','model_b'])\n    .agg(\n        total=('id','size'),\n        a_wins=('winner_model_a','sum'),\n        b_wins=('winner_model_b','sum'),\n        ties=('winner_tie','sum'),\n    )\n    .assign(\n        a_win_rate=lambda d: (d['a_wins']/d['total']).round(3),\n        b_win_rate=lambda d: (d['b_wins']/d['total']).round(3),\n        tie_rate=lambda d: (d['ties']/d['total']).round(3),\n    )\n    .reset_index()\n    .sort_values('total', ascending=False)\n)\n\ndisplay(pair_stats.head(10))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Contextualize Response with Prompt\n\nIn our approach, we will contextualize each response with the prompt instead of using a single prompt for all responses. This means that for each response, we will provide the model with the same set of prompts combined with their respective response (e.g., `(P + R_A)`, `(P + R_B)`, etc.). This approach is similar to the multiple-choice question task in NLP.\n\n> Note that some prompts and responses may not be encoded with `utf-8`, resulting in errors when creating the dataloader. In such cases, we will replace them with an empty string.\n","metadata":{"papermill":{"duration":0.014711,"end_time":"2024-06-03T17:01:31.438974","exception":false,"start_time":"2024-06-03T17:01:31.424263","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Define a function to create options based on the prompt and choices\ndef make_pairs(row):\n    def clean_text(text: any) -> str:\n        \"\"\"\n        將輸入強制轉成 str，再丟掉非法 UTF-8 字元。\n        \"\"\"\n        if not isinstance(text, str):\n            return \"\"\n        return text.encode(\"utf-8\", errors=\"ignore\") \\\n                   .decode(\"utf-8\", errors=\"ignore\")\n\n    # 一次拿到乾淨的 prompt / responses\n    prompt     = clean_text(row.get(\"prompt\", \"\"))\n    response_a = clean_text(row.get(\"response_a\", \"\"))\n    response_b = clean_text(row.get(\"response_b\", \"\"))\n\n    # （可選）判斷是否有非字串輸入\n    row[\"encode_fail\"] = any(not isinstance(row.get(col, None), str)\n                              for col in [\"prompt\", \"response_a\", \"response_b\"])\n\n    # 建立 options\n    row[\"options\"] = [\n        f\"Prompt: {prompt}\\n\\nResponse A: {response_a}\",\n        f\"Prompt: {prompt}\\n\\nResponse B: {response_b}\"\n    ]\n    return row","metadata":{"papermill":{"duration":0.023596,"end_time":"2024-06-03T17:01:31.477321","exception":false,"start_time":"2024-06-03T17:01:31.453725","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df\ndisplay(df.head(2))  # Display the first 2 rows of df\n\ntest_df = test_df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df\ndisplay(test_df.head(2))  # Display the first 2 rows of df","metadata":{"papermill":{"duration":62.732686,"end_time":"2024-06-03T17:02:34.262915","exception":false,"start_time":"2024-06-03T17:01:31.530229","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Encoding Fail Statistics\n\nLet's examine how many samples have encoding issues. From the code below, we can see that only $1\\%$ of the samples failed to be encoded, while $99\\%$ of the samples don't have any issues. A similar pattern can be expected for the test data as well. Thus, considering empty strings for this small portion of the data will not have much impact on our training and inference.","metadata":{"papermill":{"duration":0.015064,"end_time":"2024-06-03T17:02:34.293411","exception":false,"start_time":"2024-06-03T17:02:34.278347","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df.encode_fail.value_counts(normalize=False)","metadata":{"papermill":{"duration":0.032427,"end_time":"2024-06-03T17:02:34.341044","exception":false,"start_time":"2024-06-03T17:02:34.308617","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🎨 | Exploratory Data Analysis (EDA)","metadata":{"papermill":{"duration":0.015466,"end_time":"2024-06-03T17:02:34.372077","exception":false,"start_time":"2024-06-03T17:02:34.356611","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## LLM Distribution","metadata":{"papermill":{"duration":0.015944,"end_time":"2024-06-03T17:02:34.404115","exception":false,"start_time":"2024-06-03T17:02:34.388171","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import plotly.io as pio\npio.renderers.default = 'notebook_connected'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\n\n# 1. 合併並計算次數\nllm_series = pd.concat([df.model_a, df.model_b], ignore_index=True)\nllm_counts = (\n    llm_series\n    .value_counts()                      # 得到一个 Series，index 是 LLM，值是出现次数\n    .rename_axis(\"LLM\")                  # 把 index 名称设成 \"LLM\"\n    .reset_index(name=\"Count\")           # 把 Series 转成 DataFrame，并把值列命名为 \"Count\"\n)\n# 2. 排序（降冪）\nllm_counts = llm_counts.sort_values(by=\"Count\", ascending=False)\nprint(llm_counts.columns)\n\n# 3. 畫圖\nplt.figure(figsize=(12, 6))\nbars = plt.bar(llm_counts[\"LLM\"], llm_counts[\"Count\"])\nfor bar in bars:\n    h = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,\n        h,\n        f\"{h}\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=9\n    )\n\n# 4. 旋转 x 轴标签，防止重叠\nplt.xticks(rotation=45, ha=\"right\")\n\n# 5. 添加标题和坐标轴标签\nplt.title(\"Distribution of LLMs\")\nplt.xlabel(\"LLM\")\nplt.ylabel(\"Count\")\n\n# 6. 自动调整布局\nplt.tight_layout()\n\n# 7. 显示图形\nplt.show()\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.475495,"end_time":"2024-06-03T17:02:35.895386","exception":false,"start_time":"2024-06-03T17:02:34.419891","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Winning Distribution","metadata":{"papermill":{"duration":0.016453,"end_time":"2024-06-03T17:02:35.928279","exception":false,"start_time":"2024-06-03T17:02:35.911826","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 1. 計算並重構 counts DataFrame\ncounts = (\n    df[\"class_name\"]\n    .value_counts()                      # Series: index=class_name, value=frequency\n    .rename_axis(\"Winner\")               # 把 index 名稱改成 Winner\n    .reset_index(name=\"Win Count\")       # 把 value 變成 Win Count 欄位\n)\nprint(\"Columns in counts:\", counts.columns.tolist())\n\n# 2. 按 Win Count 降序排序（可选）\ncounts_sorted = counts.sort_values(by=\"Win Count\", ascending=False).reset_index(drop=True)\n\n# 3. 绘图\nplt.figure(figsize=(8, 5))\nbars = plt.bar(counts_sorted[\"Winner\"], counts_sorted[\"Win Count\"])\n\n# 4. 在柱顶标注数值\nfor bar in bars:\n    h = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,\n        h,\n        f\"{h}\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=9\n    )\n\n# 5. 旋转 x 轴标签防止重叠\nplt.xticks(rotation=45, ha=\"right\")\n\n# 6. 添加标题和坐标轴标签\nplt.title(\"Winner Distribution for Train Data\")\nplt.xlabel(\"Winner\")\nplt.ylabel(\"Win Count\")\n\n# 7. 自动调整布局并显示\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.112531,"end_time":"2024-06-03T17:02:36.056818","exception":false,"start_time":"2024-06-03T17:02:35.944287","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🔪 | Data Split\n\nIn the code snippet provided below, we will divide the existing data into training and validation using a stratification of `class_label` column.","metadata":{"papermill":{"duration":0.016208,"end_time":"2024-06-03T17:02:36.090042","exception":false,"start_time":"2024-06-03T17:02:36.073834","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, valid_df = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df[\"class_label\"],\n    shuffle=True,\n    random_state=CFG.seed,    # ← 加上這行，保證每次切分都一樣\n)\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.779715,"end_time":"2024-06-03T17:02:36.885945","exception":false,"start_time":"2024-06-03T17:02:36.10623","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🍽️ | Preprocessing\n\n**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n\n**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n\nExplore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)","metadata":{"papermill":{"duration":0.016189,"end_time":"2024-06-03T17:02:36.919019","exception":false,"start_time":"2024-06-03T17:02:36.90283","status":"completed"},"tags":[]}},{"cell_type":"code","source":"preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n    preset=CFG.preset, # Name of the model\n    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter\n)","metadata":{"papermill":{"duration":3.390834,"end_time":"2024-06-03T17:02:40.326479","exception":false,"start_time":"2024-06-03T17:02:36.935645","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, let's examine what the output shape of the preprocessing layer looks like. The output shape of the layer can be represented as $(num\\_responses, sequence\\_length)$.","metadata":{"papermill":{"duration":0.016599,"end_time":"2024-06-03T17:02:40.360813","exception":false,"start_time":"2024-06-03T17:02:40.344214","status":"completed"},"tags":[]}},{"cell_type":"code","source":"outs = preprocessor(df.options.iloc[0])  # Process options for the first row\n\n# Display the shape of each processed output\nfor k, v in outs.items():\n    print(k, \":\", v.shape)","metadata":{"papermill":{"duration":1.056025,"end_time":"2024-06-03T17:02:41.433464","exception":false,"start_time":"2024-06-03T17:02:40.377439","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We'll use the `preprocessing_fn` function to transform each text option using the `dataset.map(preprocessing_fn)` method.","metadata":{"papermill":{"duration":0.018502,"end_time":"2024-06-03T17:02:41.469384","exception":false,"start_time":"2024-06-03T17:02:41.450882","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def preprocess_fn_train(options: list[str], class_label: int):\n    \"\"\"\n    用在訓練/驗證：輸入一個 options list 和它的 class_label，\n    回傳 (model_inputs_dict, class_label)。\n    \"\"\"\n    # 安全檢查\n    if not isinstance(options, list) or len(options) == 0:\n        # full of zeros, 兩段 input_ids, attention_mask, etc.\n        empty_inputs = {\n            k: tf.zeros((len(options) or 1, CFG.sequence_length), dtype=tf.int32)\n            for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n        }\n        return empty_inputs, class_label\n\n    # 真正 call preprocessor\n    model_inputs = preprocessor(options)\n    return model_inputs, class_label\n\n\ndef preprocess_fn_test(options: list[str]):\n    \"\"\"\n    用在測試/推論：只有 options，回傳 model_inputs_dict。\n    \"\"\"\n    if not isinstance(options, list) or len(options) == 0:\n        return {\n            k: tf.zeros((len(options) or 1, CFG.sequence_length), dtype=tf.int32)\n            for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n        }\n    return preprocessor(options)\n","metadata":{"papermill":{"duration":0.024249,"end_time":"2024-06-03T17:02:41.510214","exception":false,"start_time":"2024-06-03T17:02:41.485965","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🍚 | DataLoader\n\nThe code below sets up a robust data flow pipeline using `tf.data.Dataset` for data processing. Notable aspects of `tf.data` include its ability to simplify pipeline construction and represent components in sequences.\n\nTo learn more about `tf.data`, refer to this [documentation](https://www.tensorflow.org/guide/data).","metadata":{"papermill":{"duration":0.016686,"end_time":"2024-06-03T17:02:41.543394","exception":false,"start_time":"2024-06-03T17:02:41.526708","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_dataset(options, labels=None, batch_size=32,\n                  cache=True, shuffle_size=1024):\n    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n     # 1. 建立原始 ds，並選擇對應的 map function\n    if labels is not None:\n        ds = tf.data.Dataset.from_tensor_slices((options, labels))\n        map_fn = preprocess_fn_train\n    else:\n        ds = tf.data.Dataset.from_tensor_slices(options)\n        map_fn = preprocess_fn_test\n\n    # 2. Shuffle（只在有 labels 時，且 shuffle_size > 0）\n    if labels is not None and shuffle_size > 0:\n        ds = ds.shuffle(shuffle_size, seed=CFG.seed, reshuffle_each_iteration=True)\n\n    # 3. Cache（可選）\n    if cache:\n        ds = ds.cache()\n\n    # 4. Map → preprocessing\n    ds = ds.map(map_fn, num_parallel_calls=AUTO)\n\n    # 5. Batch & Prefetch\n    ds = ds.batch(batch_size, drop_remainder=False)\n    ds = ds.prefetch(AUTO)\n\n    return ds","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.026326,"end_time":"2024-06-03T17:02:41.586658","exception":false,"start_time":"2024-06-03T17:02:41.560332","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build Train/Valid Dataloader","metadata":{"papermill":{"duration":0.016559,"end_time":"2024-06-03T17:02:41.619713","exception":false,"start_time":"2024-06-03T17:02:41.603154","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train\ntrain_texts = train_df[\"options\"].tolist()\ntrain_labels = train_df[\"class_label\"].tolist()\ntrain_ds = build_dataset(\n    train_texts,\n    train_labels,\n    batch_size=CFG.batch_size,\n    cache=True,       # cache 訓練集以加速多 epoch 重跑\n    shuffle_size=1024      # shuffle buffer size\n)\n\n# Valid\nvalid_texts = valid_df[\"options\"].tolist()\nvalid_labels = valid_df[\"class_label\"].tolist()\nvalid_ds = build_dataset(\n    valid_texts,\n    valid_labels,\n    batch_size=CFG.batch_size,\n    cache=False,      # 驗證集不需要 cache\n    shuffle_size=0         # 不做 shuffle\n)\n","metadata":{"_kg_hide-input":false,"papermill":{"duration":5.308478,"end_time":"2024-06-03T17:02:46.944786","exception":false,"start_time":"2024-06-03T17:02:41.636308","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ⚓ | LR Schedule\n\nImplementing a learning rate scheduler is crucial for transfer learning. The learning rate initiates at `lr_start` and gradually tapers down to `lr_min` using various techniques, including:\n- `step`: Lowering the learning rate in step-wise manner resembling stairs.\n- `cos`: Utilizing a cosine curve to gradually reduce the learning rate.\n- `exp`: Exponentially decreasing the learning rate.\n\n**Importance:** A well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation.","metadata":{"papermill":{"duration":0.017157,"end_time":"2024-06-03T17:02:46.979278","exception":false,"start_time":"2024-06-03T17:02:46.962121","status":"completed"},"tags":[]}},{"cell_type":"code","source":"'''\nimport math\n\ndef get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6\n    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8\n\n    def lrfn(epoch):  # Learning rate update function\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n        elif mode == 'cos':\n            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n        return lr\n\n    if plot:  # Plot lr curve if plot is True\n        plt.figure(figsize=(10, 5))\n        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('lr')\n        plt.title('LR Scheduler')\n        plt.show()\n\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback\n'''\nimport tensorflow as tf\n\ndef get_cosine_warmup_schedule(steps_per_epoch: int):\n    total_steps   = CFG.epochs * steps_per_epoch\n    warmup_steps  = int(total_steps * CFG.warmup_ratio)\n    # 1. 先做 warmup：从 0 → learning_rate\n    warmup_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n        initial_learning_rate=0.0,\n        decay_steps=warmup_steps,\n        end_learning_rate=CFG.learning_rate,\n        power=1.0\n    )\n    # 2. 再做剩余步数的 cosine decay：从 learning_rate → weight_decay（当作最小 lr）\n    cosine_fn = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=CFG.learning_rate,\n        decay_steps=total_steps - warmup_steps,\n        alpha=CFG.weight_decay/CFG.learning_rate  # α = lr_min / lr_max\n    )\n    # 3. 用 `tf.where` 把两个 schedule 拼起来\n    def lr_schedule_fn(step):\n        return tf.where(\n            step < warmup_steps,\n            warmup_fn(step),\n            cosine_fn(step - warmup_steps)\n        )\n    return lr_schedule_fn\n\n\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.030355,"end_time":"2024-06-03T17:02:47.026587","exception":false,"start_time":"2024-06-03T17:02:46.996232","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lr_cb = get_lr_callback(CFG.batch_size, plot=True)\nfrom tensorflow.keras.optimizers import AdamW\n\n\nsteps_per_epoch = len(train_df) // CFG.batch_size\nlr_fn = get_cosine_warmup_schedule(steps_per_epoch)\n\n# 使用內建的 AdamW\noptimizer = AdamW(\n    learning_rate=lr_fn,\n    weight_decay=CFG.weight_decay\n)\n'''\nmodel.compile(\n    optimizer=optimizer,\n    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")],\n)\n'''","metadata":{"papermill":{"duration":0.302348,"end_time":"2024-06-03T17:02:47.345856","exception":false,"start_time":"2024-06-03T17:02:47.043508","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 💾 | Model Checkpointing\n\nThe following code will create a callback that will save the best checkpoint of the model during training, which we will use for inference in the submission.","metadata":{"papermill":{"duration":0.017351,"end_time":"2024-06-03T17:02:47.381369","exception":false,"start_time":"2024-06-03T17:02:47.364018","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 把 checkpoint 存到專案的 checkpoints 目錄，先確保它存在\nckpt_dir = Path(\"checkpoints\")\nckpt_dir.mkdir(exist_ok=True)\n\nckpt_cb = keras.callbacks.ModelCheckpoint(\n    filepath=str(ckpt_dir / \"best_model.{epoch:02d}-{val_loss:.4f}.weights.h5\"),\n    monitor=\"val_loss\",             # 要對應 compile 時的 loss 名稱\n    save_best_only=True,\n    save_weights_only=True,          # 只存 weights\n    mode=\"min\",                      # loss 越低代表越好\n    verbose=1                        # 儲存時顯示訊息\n)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.025064,"end_time":"2024-06-03T17:02:47.424145","exception":false,"start_time":"2024-06-03T17:02:47.399081","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📏 | Metric\n\nThe metric for this competition is **Log Loss**. This metric can be expressed mathematically as,\n\n$$\n\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right)\n$$\n\nwhere $ N $ is the number of samples, $ y_i $ is the true label, and $ p_i $ is the predicted probability of the sample belonging to the positive class.\n\nNote that this metric is similar to categorical cross entropy widely used in classification tasks. Thus, we don't need to implement the loss from scratch. As the Keras library already has an implementation of this metric, we will simply use the metric to monitor performance of our model.\n","metadata":{"papermill":{"duration":0.017033,"end_time":"2024-06-03T17:02:47.458564","exception":false,"start_time":"2024-06-03T17:02:47.441531","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#log_loss = keras.metrics.CategoricalCrossentropy(name=\"log_loss\")","metadata":{"papermill":{"duration":0.043053,"end_time":"2024-06-03T17:02:47.519019","exception":false,"start_time":"2024-06-03T17:02:47.475966","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🤖 | Modeling\n\nThe `KerasNLP` library provides various NLP model architectures such as `Bert`, `Roberta`, `DebertaV3`, and more. While this notebook focuses on `DebertaV3`, you can explore others in the [KerasNLP documentation](https://keras.io/api/keras_nlp/models/). For a deeper understanding, refer to the [getting started guide](https://keras.io/guides/keras_nlp/getting_started/).\n\nOur approach utilizes `keras_nlp.models.DebertaV3Classifier` to process each prompt and response pair, generating output embeddings. We then concatenate these embeddings and pass them through a Pooling layer and a classifier to obtain logits, followed by a `softmax` function for the final output.\n\nWhen dealing with multiple responses, we use a weight-sharing strategy. This means we provide the model with one response at a time along with the prompt `(P + R_A)`, `(P + R_B)`, etc., using the same model weights for all responses. After obtaining embeddings for all responses, we concatenate them and apply average pooling. Next, we use a `Linear/Dense` layer along with the `Softmax` function as the classifier for the final result. Providing all responses at once would increase text length and complicate model handling. Note that, in the classifier, we use 3 classes for `winner_model_a`, `winner_model_b`, and `draw` cases.\n\nThe diagram below illustrates this approach:\n\n<div align=\"center\">\n    <img src=\"https://i.postimg.cc/g0gcvy3f/Kaggle-drawio.png\">\n</div>\n\nFrom a coding perspective, note that we use the same model for all responses with shared weights, contrary to the separate models implied in the diagram.","metadata":{"papermill":{"duration":0.017265,"end_time":"2024-06-03T17:02:47.553968","exception":false,"start_time":"2024-06-03T17:02:47.536703","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Define input layers\n'''\ninputs = {\n    \"token_ids\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"token_ids\"),\n    \"padding_mask\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"padding_mask\"),\n}\n# Create a DebertaV3Classifier backbone\nbackbone = keras_nlp.models.DebertaV3Backbone.from_preset(\n    CFG.preset,\n)\n\n# Compute embeddings for first response: (P + R_A) using backbone\nresponse_a = {k: v[:, 0, :] for k, v in inputs.items()}\nembed_a = backbone(response_a)\n\n# Compute embeddings for second response: (P + R_B), using the same backbone\nresponse_b = {k: v[:, 1, :] for k, v in inputs.items()}\nembed_b = backbone(response_b)\n\n# Compute final output\nembeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])\nembeds = keras.layers.GlobalAveragePooling1D()(embeds)\noutputs = keras.layers.Dense(3, activation=\"softmax\", name=\"classifier\")(embeds)\nmodel = keras.Model(inputs, outputs)\n\n# Compile the model with optimizer, loss, and metrics\nmodel.compile(\n    optimizer=keras.optimizers.Adam(5e-6),\n    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),\n    metrics=[\n        log_loss,\n        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n    ],\n)\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\ncallbacks = [\n    EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True),\n    ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n]\n'''\nimport math\nfrom tensorflow.keras import mixed_precision, regularizers\nfrom tensorflow.keras.optimizers.schedules import LearningRateSchedule\n\n# 1) Mixed precision\nmixed_precision.set_global_policy(\"mixed_float16\")\n\n# 2) Prepare checkpoint dir\nckpt_dir = Path(\"checkpoints\")\nckpt_dir.mkdir(exist_ok=True)\n\n# 3) Model architecture\ninputs = {\n    \"input_ids\": keras.Input((2, CFG.sequence_length), tf.int32, name=\"input_ids\"),\n    \"attention_mask\": keras.Input((2, CFG.sequence_length), tf.int32, name=\"attention_mask\"),\n}\n\nbackbone = keras_nlp.models.DebertaV3Backbone.from_preset(CFG.preset)\nbackbone.trainable = False\n\ndef slice_pair_and_rename(d, idx):\n    return {\n        \"token_ids\":    d[\"input_ids\"][:, idx, :],\n        \"padding_mask\": d[\"attention_mask\"][:, idx, :]\n    }\n\na = backbone(slice_pair_and_rename(inputs, 0))\nb = backbone(slice_pair_and_rename(inputs, 1))\n\nx = keras.layers.Concatenate(axis=1)([a, b])\nx = keras.layers.GlobalAveragePooling1D()(x)\n# add extra Dense + Dropout per updated CFG.dropout_rate\nx = keras.layers.Dense(\n    256,\n    activation=\"relu\",\n    kernel_regularizer=regularizers.l2(CFG.weight_decay)\n)(x)\nx = keras.layers.Dropout(CFG.dropout_rate)(x)\noutputs = keras.layers.Dense(\n    len(CFG.class_names),\n    activation=\"softmax\",\n    dtype=\"float32\",\n    kernel_regularizer=regularizers.l2(CFG.weight_decay),\n    name=\"classifier\"\n)(x)\n\nmodel = keras.Model(inputs, outputs)\n\n# 4) Cosine-with-warmup schedule\nclass CosineWarmupSchedule(LearningRateSchedule):\n    def __init__(self, lr_max, lr_min, warmup_steps, total_steps):\n        super().__init__()\n        self.lr_max = lr_max; self.lr_min = lr_min\n        self.warmup_steps = warmup_steps; self.total_steps = total_steps\n\n    def __call__(self, step):\n        step = tf.cast(step, tf.float32)\n        ws = tf.cast(self.warmup_steps, tf.float32)\n        ts = tf.cast(self.total_steps, tf.float32)\n        warm = self.lr_max * (step / ws)\n        dec_step = step - ws\n        dec_steps = ts - ws\n        cosine = 0.5 * (1 + tf.cos(math.pi * dec_step / dec_steps))\n        decay_lr = self.lr_min + (self.lr_max - self.lr_min) * cosine\n        return tf.where(step < ws, warm, decay_lr)\n\n# compute total_steps & warmup_steps using updated CFG\nsteps_per_epoch = len(train_df) // CFG.batch_size\ntotal_steps   = (CFG.head_only_epochs + CFG.fine_tune_epochs) * steps_per_epoch\nwarmup_steps  = int(CFG.head_only_epochs * steps_per_epoch * CFG.warmup_ratio)\n\nlr_schedule = CosineWarmupSchedule(\n    CFG.learning_rate, CFG.weight_decay, warmup_steps, total_steps\n)\n\n# 5) class weights\ncounts = train_df[\"class_label\"].value_counts().to_dict()\ntotal  = len(train_df)\nclass_weight = {k: total/v for k, v in counts.items()}\n\n# 6) Stage 1 callbacks\ncallbacks_stage1 = [\n    EarlyStopping(monitor=\"val_loss\", patience=CFG.head_only_epochs,\n                  restore_best_weights=True, verbose=1),\n    ModelCheckpoint(filepath=str(ckpt_dir/\"best_head.weights.h5\"),\n                    monitor=\"val_loss\", save_best_only=True,\n                    save_weights_only=True, verbose=1)\n]\n\n# 7) Stage 1: train head only\noptimizer1 = AdamW(learning_rate=lr_schedule, weight_decay=CFG.weight_decay)\nmodel.compile(\n    optimizer=optimizer1,\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n)\nprint(f\" Stage1: training head for {CFG.head_only_epochs} epochs\")\nhistory_head = model.fit(\n    train_ds,\n    validation_data=valid_ds,\n    epochs=CFG.head_only_epochs,\n    class_weight=class_weight,\n    callbacks=callbacks_stage1\n)\n\n# 8) Unfreeze more layers & Stage 2 callbacks\nbackbone.trainable = True\nfor layer in backbone.layers[:-CFG.unfrozen_backbone_layers]:\n    layer.trainable = False\n\ncallbacks_stage2 = [\n    EarlyStopping(monitor=\"val_loss\", patience=CFG.fine_tune_epochs,\n                  restore_best_weights=True, verbose=1),\n    ModelCheckpoint(filepath=str(ckpt_dir/\n        f\"best_finetuned.epoch{{epoch:02d}}_val{{val_loss:.4f}}.weights.h5\"),\n                    monitor=\"val_loss\", mode=\"min\",\n                    save_best_only=True, save_weights_only=True, verbose=1),\n    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5,\n                      patience=2, min_lr=1e-7, verbose=1)\n]\n\n# 9) Stage 2: fine-tune backbone\noptimizer2 = AdamW(learning_rate=CFG.learning_rate * 0.1,\n                   weight_decay=CFG.weight_decay)\nmodel.compile(\n    optimizer=optimizer2,\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n)\nprint(f\" Stage2: fine-tuning last {CFG.unfrozen_backbone_layers} layers \"\n      f\"for {CFG.fine_tune_epochs} epochs\")\nhistory_ft = model.fit(\n    train_ds,\n    validation_data=valid_ds,\n    initial_epoch=CFG.head_only_epochs,\n    epochs=CFG.head_only_epochs + CFG.fine_tune_epochs,\n    class_weight=class_weight,\n    callbacks=callbacks_stage2\n)","metadata":{"papermill":{"duration":8.231591,"end_time":"2024-06-03T17:02:55.802843","exception":false,"start_time":"2024-06-03T17:02:47.571252","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Summary","metadata":{"papermill":{"duration":0.018012,"end_time":"2024-06-03T17:02:55.83988","exception":false,"start_time":"2024-06-03T17:02:55.821868","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model.summary()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.051632,"end_time":"2024-06-03T17:02:55.909391","exception":false,"start_time":"2024-06-03T17:02:55.857759","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Plot\n\nIn the model graph below, it may seem there are **four** inputs, but actually, there are **two** as discussed before. Our input consists of two parts, one for each response. However, for each input, we have `token_ids` and `padding_mask`, which makes it look like we have four inputs, but in reality, we have two inputs.","metadata":{"papermill":{"duration":0.018914,"end_time":"2024-06-03T17:02:55.947016","exception":false,"start_time":"2024-06-03T17:02:55.928102","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Currently throwing error !! [probably library or env issue, so hopefully will be fixed soon]\n\n# keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.025569,"end_time":"2024-06-03T17:02:55.991355","exception":false,"start_time":"2024-06-03T17:02:55.965786","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🚂 | Training","metadata":{"papermill":{"duration":0.018413,"end_time":"2024-06-03T17:02:56.028558","exception":false,"start_time":"2024-06-03T17:02:56.010145","status":"completed"},"tags":[]}},{"cell_type":"code","source":"'''\nimport wandb\n#from wandb.keras import WandbCallback\n\n# 初始化 wandb 專案（在 fit() 之前）\nwandb.init(project=\"kerasnlp-training\", name=\"deberta-run\")\n\n# 開始訓練模型，同步記錄到 wandb\nhistory = model.fit(\n    train_ds,\n    epochs=CFG.epochs,\n    validation_data=valid_ds,\n    callbacks=[lr_cb, ckpt_cb, WandbCallback()]\n)\n'''\n''''\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\ncallbacks = [\n    EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True),\n    ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n]\n'''\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":8128.14365,"end_time":"2024-06-03T19:18:24.191101","exception":false,"start_time":"2024-06-03T17:02:56.047451","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start training the model\n'''\n# 開始訓練\nhistory = model.fit(\n    train_ds,\n    validation_data=valid_ds,\n    epochs=CFG.epochs,\n    callbacks=callbacks,   # <- 直接引用統一好的 callbacks 列表\n)\n'''\n'''\n# Stage 1: 只训练 Head\nhistory_head = model.fit(\n    train_ds,\n    validation_data=valid_ds,\n    epochs=CFG.epochs,\n    callbacks=callbacks,   # <- 直接引用統一好的 callbacks 列表\n)\n# Stage 2: 解冻微调\nhistory_ft   = model.fit(\n    train_ds,\n    validation_data=valid_ds,\n    epochs=CFG.epochs,\n    callbacks=callbacks,   # <- 直接引用統一好的 callbacks 列表\n)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# 1. 从两个 history 对象里合并指标\nloss = history_head.history['loss'] + history_ft.history['loss']\nval_loss = history_head.history.get('val_loss', []) + history_ft.history.get('val_loss', [])\n\nacc = history_head.history.get('sparse_categorical_accuracy', history_head.history.get('accuracy', [])) \\\n      + history_ft.history.get('sparse_categorical_accuracy', history_ft.history.get('accuracy', []))\nval_acc = history_head.history.get('val_sparse_categorical_accuracy', history_head.history.get('val_accuracy', [])) \\\n          + history_ft.history.get('val_sparse_categorical_accuracy', history_ft.history.get('val_accuracy', []))\n\n# 2. x 轴：总共的 epoch 数\nepochs = list(range(1, len(loss) + 1))\n\n# 3. 绘图\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# 左：Loss\naxes[0].plot(epochs, loss, label='Train Loss')\nif val_loss:\n    axes[0].plot(epochs, val_loss, label='Validation Loss')\naxes[0].axvline(x=CFG.head_only_epochs + 0.5, color='gray', linestyle='--',\n                label='Unfreeze Backbone')  # 标出解冻点\naxes[0].set_title('Loss over Epochs')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].legend()\n\n# 右：Accuracy\naxes[1].plot(epochs, acc, label='Train Accuracy')\nif val_acc:\n    axes[1].plot(epochs, val_acc, label='Validation Accuracy')\naxes[1].axvline(x=CFG.head_only_epochs + 0.5, color='gray', linestyle='--',\n                label='Unfreeze Backbone')\naxes[1].set_title('Accuracy over Epochs')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Best Model\n\nAfter training, let's load the weight with best result to get the best performance.","metadata":{"papermill":{"duration":0.805618,"end_time":"2024-06-03T19:18:25.735457","exception":false,"start_time":"2024-06-03T19:18:24.929839","status":"completed"},"tags":[]}},{"cell_type":"code","source":"'''\nckpt_dir = Path(\"checkpoints\")\n# 找到最新一次儲存的 checkpoint\nlatest_ckpt = tf.train.latest_checkpoint(str(ckpt_dir))\nprint(\"Loading weights from:\", latest_ckpt)\nmodel.load_weights(latest_ckpt)\n'''\n# 1. 指定 checkpoint 目录\nckpt_dir = Path(\"checkpoints\")\nif not ckpt_dir.exists():\n    raise FileNotFoundError(f\"Checkpoint directory not found: {ckpt_dir}\")\n\n# 2. 列出所有 .weights.h5 文件（包括 head-only 与 finetuned）\nweight_files = list(ckpt_dir.glob(\"*.weights.h5\"))\nif not weight_files:\n    raise FileNotFoundError(f\"No .weights.h5 files found in {ckpt_dir}\")\n\n# 3. 从文件名中解析出 epoch 数字；head-only 文件设为 0\ndef extract_epoch(fp: Path) -> int:\n    m = re.search(r\"epoch(\\d+)\", fp.name)\n    return int(m.group(1)) if m else 0\n\n# 4. 选出 epoch 最大的那个文件\nlatest_file = max(weight_files, key=extract_epoch)\n\n# 5. 加载权重\nmodel.load_weights(str(latest_file))\nprint(f\"Loaded weights from: {latest_file}\")","metadata":{"papermill":{"duration":2.546342,"end_time":"2024-06-03T19:18:29.025014","exception":false,"start_time":"2024-06-03T19:18:26.478672","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🧪 | Prediction","metadata":{"papermill":{"duration":0.754041,"end_time":"2024-06-03T19:18:30.53816","exception":false,"start_time":"2024-06-03T19:18:29.784119","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Build test dataset\ntest_texts = test_df[\"options\"].tolist()\n\ntest_ds = build_dataset(\n    options=test_texts,\n    labels=None,                     # 明确告诉它没有 labels\n    batch_size=CFG.batch_size,       # 或者 min(len(test_df), CFG.batch_size)\n    cache=False,                     # 不 cache\n    shuffle_size=0                   # 不 shuffle\n)\n","metadata":{"papermill":{"duration":1.156834,"end_time":"2024-06-03T19:18:32.520369","exception":false,"start_time":"2024-06-03T19:18:31.363535","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions using the trained model on test data\ntest_preds = model.predict(test_ds, verbose=1)","metadata":{"papermill":{"duration":6.510153,"end_time":"2024-06-03T19:18:39.767367","exception":false,"start_time":"2024-06-03T19:18:33.257214","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📬 | Submission\n\nFollowing code will prepare the submission file.","metadata":{"papermill":{"duration":0.768879,"end_time":"2024-06-03T19:18:41.329256","exception":false,"start_time":"2024-06-03T19:18:40.560377","status":"completed"},"tags":[]}},{"cell_type":"code","source":"'''\nsub_df = test_df[[\"id\"]].copy()\nsub_df[CFG.class_names] = test_preds.tolist()\nsub_df.to_csv(\"submission.csv\", index=False)\nsub_df.head()\n'''\n# 1. 把預測結果做成 DataFrame\npreds_df = pd.DataFrame(test_preds, columns=CFG.class_names)\n\n# 2. 合併 id\nsub_df = pd.concat([test_df[\"id\"].reset_index(drop=True), preds_df], axis=1)\n\n# 3. （可選）加最終預測類別\nsub_df[\"pred_class\"] = preds_df.idxmax(axis=1)\n\n# 4. 寫檔並輸出確認\nsub_df.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv with columns:\", sub_df.columns.tolist())\nsub_df.head()","metadata":{"papermill":{"duration":0.751809,"end_time":"2024-06-03T19:18:42.839827","exception":false,"start_time":"2024-06-03T19:18:42.088018","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🔭 | Future Directions\n\nIn this notebook, we've achieved a good score with a small model and modest token length. But there's plenty of room to improve. Here's how:\n\n1. Try bigger models like `Deberta-Base` or `Deberta-Small`, or even LLMs like `Gemma`.\n2. Increase max token length to reduce loss of data.\n3. Use a five-fold cross-validation and ensemble to make the model robust and get better scores.\n4. Add augmentation like shuffling response orders for more robust performance.\n5. Train for more epochs.\n6. Tune the learning rate scheduler.","metadata":{"papermill":{"duration":0.740084,"end_time":"2024-06-03T19:18:44.386408","exception":false,"start_time":"2024-06-03T19:18:43.646324","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 📌 | Reference\n\n* [LLM Science Exam: KerasCore + KerasNLP [TPU]](https://www.kaggle.com/code/awsaf49/llm-science-exam-kerascore-kerasnlp-tpu)\n* [AES 2.0: KerasNLP Starter](https://www.kaggle.com/code/awsaf49/aes-2-0-kerasnlp-starter)","metadata":{"papermill":{"duration":0.830152,"end_time":"2024-06-03T19:18:45.950588","exception":false,"start_time":"2024-06-03T19:18:45.120436","status":"completed"},"tags":[]}}]}